

<!DOCTYPE html>
<html>
<meta charset="UTF-8">
<style>
@media (prefers-color-scheme: dark) {
    body {
        background-color: #1c1c1c;
        color: white;
    }
    .markdown-body table tr {
        background-color: #1c1c1c;
    }
    .markdown-body table tr:nth-child(2n) {
        background-color: black;
    }
}
</style>



<link rel="alternate" type="application/rss+xml" href="../../../../feed.xml" title="Understanding PLONK">



<link rel="stylesheet" type="text/css" href="../../../../css/common-vendor.b8ecfc406ac0b5f77a26.css">
<link rel="stylesheet" type="text/css" href="../../../../css/fretboard.f32f2a8d5293869f0195.css">
<link rel="stylesheet" type="text/css" href="../../../../css/pretty.0ae3265014f89d9850bf.css">
<link rel="stylesheet" type="text/css" href="../../../../css/pretty-vendor.83ac49e057c3eac4fce3.css">
<link rel="stylesheet" type="text/css" href="../../../../css/global.css">
<link rel="stylesheet" type="text/css" href="../../../../css/misc.css">

<script type="text/x-mathjax-config">
<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\(', '\)']]
  },
  svg: {
    fontCache: 'global',
  }
};
</script>
<script type="text/javascript" id="MathJax-script" async
  src="../../../../scripts/tex-svg.js">
</script>

<style>
</style>

<div id="doc" class="container-fluid markdown-body comment-enabled" data-hard-breaks="true">

<div id="color-mode-switch">
  <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2">
    <path stroke-linecap="round" stroke-linejoin="round" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364l-.707-.707M6.343 6.343l-.707-.707m12.728 0l-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z" />
  </svg>
  <input type="checkbox" id="switch" />
  <label for="switch">Dark Mode Toggle</label>
  <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2">
    <path stroke-linecap="round" stroke-linejoin="round" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003 0 0012 21a9.003 9.003 0 008.354-5.646z" />
  </svg>
</div>

<script type="text/javascript">
  // Update root html class to set CSS colors
  const toggleDarkMode = () => {
    const root = document.querySelector('html');
    root.classList.toggle('dark');
  }

  // Update local storage value for colorScheme
  const toggleColorScheme = () => {
    const colorScheme = localStorage.getItem('colorScheme');
    if (colorScheme === 'light') localStorage.setItem('colorScheme', 'dark');
    else localStorage.setItem('colorScheme', 'light');
  }

  // Set toggle input handler
  const toggle = document.querySelector('#color-mode-switch input[type="checkbox"]');
  if (toggle) toggle.onclick = () => {
    toggleDarkMode();
    toggleColorScheme();
  }

  // Check for color scheme on init
  const checkColorScheme = () => {
    const colorScheme = localStorage.getItem('colorScheme');
    // Default to light for first view
    if (colorScheme === null || colorScheme === undefined) localStorage.setItem('colorScheme', 'light');
    // If previously saved to dark, toggle switch and update colors
    if (colorScheme === 'dark') {
      toggle.checked = true;
      toggleDarkMode();
    }
  }
  checkColorScheme();
</script>

<meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="Understanding PLONK" />
<meta name="twitter:image" content="http://vitalik.eth.limo/images/icon.png" />


<br>
<h1 style="margin-bottom:7px"> Understanding PLONK </h1>
<small style="float:left; color: #888"> 2019 Sep 22 </small>
<small style="float:right; color: #888"><a href="../../../../index.html">See all posts</a></small>
<br> <br> <br>
<title> Understanding PLONK </title>

<p><em>Special thanks to Justin Drake, Karl Floersch, Hsiao-wei Wang,
Barry Whitehat, Dankrad Feist, Kobi Gurkan and Zac Williamson for
review</em></p>
<p>Very recently, Ariel Gabizon, Zac Williamson and Oana Ciobotaru
announced a new general-purpose zero-knowledge proof scheme called <a
href="https://eprint.iacr.org/2019/953">PLONK</a>, standing for the
unwieldy quasi-backronym "Permutations over Lagrange-bases for
Oecumenical Noninteractive arguments of Knowledge". While <a
href="https://eprint.iacr.org/2016/260.pdf">improvements</a> to
general-purpose <a
href="https://arxiv.org/abs/1903.12243">zero-knowledge proof</a>
protocols have been <a href="https://dci.mit.edu/zksharks">coming</a>
for <a href="https://eprint.iacr.org/2017/1066">years</a>, what PLONK
(and the earlier but more complex <a
href="https://www.benthamsgaze.org/2019/02/07/introducing-sonic-a-practical-zk-snark-with-a-nearly-trustless-setup/">SONIC</a>
and the more recent <a
href="https://eprint.iacr.org/2019/1047.pdf">Marlin</a>) bring to the
table is a series of enhancements that may greatly improve the usability
and progress of these kinds of proofs in general.</p>
<p>The first improvement is that while PLONK still requires a trusted
setup procedure similar to that needed for the <a
href="https://minezcash.com/zcash-trusted-setup/">SNARKs in Zcash</a>,
it is a "universal and updateable" trusted setup. This means two things:
first, instead of there being one separate trusted setup for every
program you want to prove things about, there is one single trusted
setup for the whole scheme after which you can use the scheme with any
program (up to some maximum size chosen when making the setup). Second,
there is a way for multiple parties to participate in the trusted setup
such that it is secure as long as any one of them is honest, and this
multi-party procedure is fully sequential: first one person
participates, then the second, then the third... The full set of
participants does not even need to be known ahead of time; new
participants could just add themselves to the end. This makes it easy
for the trusted setup to have a large number of participants, making it
quite safe in practice.</p>
<p>The second improvement is that the "fancy cryptography" it relies on
is one single standardized component, called a "polynomial commitment".
PLONK uses "Kate commitments", based on a trusted setup and elliptic
curve pairings, but you can instead swap it out with other schemes, such
as <a href="../../../2017/11/22/starks_part_2.html">FRI</a> (which would
<a href="https://eprint.iacr.org/2019/1020">turn PLONK into a kind of
STARK</a>) or DARK (based on hidden-order groups). This means the scheme
is theoretically compatible with any (achievable) tradeoff between proof
size and security assumptions.</p>
<center>
<img src="../../../../images/plonk-files/Tradeoffs.png" class="padded" />
</center>
<p><br></p>
<p>What this means is that use cases that require different tradeoffs
between proof size and security assumptions (or developers that have
different ideological positions about this question) can still share the
bulk of the same tooling for "arithmetization" - the process for
converting a program into a set of polynomial equations that the
polynomial commitments are then used to check. If this kind of scheme
becomes widely adopted, we can thus expect rapid progress in improving
shared arithmetization techniques.</p>
<h2 id="how-plonk-works">How PLONK works</h2>
<p>Let us start with an explanation of how PLONK works, in a somewhat
abstracted format that focuses on polynomial equations without
immediately explaining how those equations are verified. A key
ingredient in PLONK, as is the case in the <a
href="https://medium.com/@VitalikButerin/quadratic-arithmetic-programs-from-zero-to-hero-f6d558cea649">QAPs
used in SNARKs</a>, is a procedure for converting a problem of the form
"give me a value <span class="math inline">\(X\)</span> such that a
specific program <span class="math inline">\(P\)</span> that I give you,
when evaluated with <span class="math inline">\(X\)</span> as an input,
gives some specific result <span class="math inline">\(Y\)</span>" into
the problem "give me a set of values that satisfies a set of math
equations". The program <span class="math inline">\(P\)</span> can
represent many things; for example the problem could be "give me a
solution to this sudoku", which you would encode by setting <span
class="math inline">\(P\)</span> to be a sudoku verifier plus some
initial values encoded and setting <span
class="math inline">\(Y\)</span> to <span
class="math inline">\(1\)</span> (ie. "yes, this solution is correct"),
and a satisfying input <span class="math inline">\(X\)</span> would be a
valid solution to the sudoku. This is done by representing <span
class="math inline">\(P\)</span> as a circuit with logic gates for
addition and multiplication, and converting it into a system of
equations where the variables are the values on all the wires and there
is one equation per gate (eg. <span class="math inline">\(x_6 = x_4
\cdot x_7\)</span> for multiplication, <span class="math inline">\(x_8 =
x_5 + x_9\)</span> for addition).</p>
<p>Here is an example of the problem of finding <span
class="math inline">\(x\)</span> such that <span
class="math inline">\(P(x) = x^3 + x + 5 = 35\)</span> (hint: <span
title="Though other solutions also exist over fields where -31 has a square root; since SNARKs are done over prime fields this is something to watch out for!"><span
class="math inline">\(x = 3\)</span></span>):</p>
<center>
<img src="../../../../images/plonk-files/Circuit.png" class="padded" />
</center>
<p><br></p>
<p>We can label the gates and wires as follows:</p>
<center>
<img src="../../../../images/plonk-files/Circuit2.png" class="padded" />
</center>
<p><br></p>
<p>On the gates and wires, we have two types of constraints:
<strong>gate constraints</strong> (equations between wires attached to
the same gate, eg. <span class="math inline">\(a_1 \cdot b_1 =
c_1\)</span>) and <strong>copy constraints</strong> (claims about
equality of different wires anywhere in the circuit, eg. <span
class="math inline">\(a_0 = a_1 = b_1 = b_2 = a_3\)</span> or <span
class="math inline">\(c_0 = a_1\)</span>). We will need to create a
structured system of equations, which will ultimately reduce to a very
small number of polynomial equations, to represent both.</p>
<p>In PLONK, the setup for these equations is as follows. Each equation
is of the following form (think: <span class="math inline">\(L\)</span>
= left, <span class="math inline">\(R\)</span> = right, <span
class="math inline">\(O\)</span> = output, <span
class="math inline">\(M\)</span> = multiplication, <span
class="math inline">\(C\)</span> = constant):</p>
<p><span class="math display">\[
\left(Q_{L_{i}}\right) a_{i}+\left(Q_{R_{i}}\right)
b_{i}+\left(Q_{O_{i}}\right) c_{i}+\left(Q_{M_{i}}\right) a_{i}
b_{i}+Q_{C_{i}}=0
\]</span></p>
<p>Each <span class="math inline">\(Q\)</span> value is a constant; the
constants in each equation (and the number of equations) will be
different for each program. Each small-letter value is a variable,
provided by the user: <span class="math inline">\(a_i\)</span> is the
left input wire of the <span class="math inline">\(i\)</span>'th gate,
<span class="math inline">\(b_i\)</span> is the right input wire, and
<span class="math inline">\(c_i\)</span> is the output wire of the <span
class="math inline">\(i\)</span>'th gate. For an addition gate, we
set:</p>
<p><span class="math display">\[
Q_{L_{i}}=1, Q_{R_{i}}=1, Q_{M_{i}}=0, Q_{O_{i}}=-1, Q_{C_{i}}=0
\]</span></p>
<p>Plugging these constants into the equation and simplifying gives us
<span class="math inline">\(a_i + b_i - c_i = 0\)</span>, which is
exactly the constraint that we want. For a multiplication gate, we
set:</p>
<p><span class="math display">\[
Q_{L_{i}}=0, Q_{R_{i}}=0, Q_{M_{i}}=1, Q_{O_{i}}=-1, Q_{C_{i}}=0
\]</span></p>
<p>For a constant gate setting <span class="math inline">\(a_i\)</span>
to some constant <span class="math inline">\(x\)</span>, we set:</p>
<p><span class="math display">\[
Q_{L}=1, Q_{R}=0, Q_{M}=0, Q_{O}=0, Q_{C}=-x
\]</span></p>
<p>You may have noticed that each end of a wire, as well as each wire in
a set of wires that clearly must have the same value (eg. <span
class="math inline">\(x\)</span>), corresponds to a distinct variable;
there's nothing so far forcing the output of one gate to be the same as
the input of another gate (what we call "copy constraints"). PLONK does
of course have a way of enforcing copy constraints, but we'll get to
this later. So now we have a problem where a prover wants to prove that
they have a bunch of <span class="math inline">\(x_{a_i},
x_{b_i}\)</span> and <span class="math inline">\(x_{c_i}\)</span> values
that satisfy a bunch of equations that are of the same form. This is
still a big problem, but unlike "find a satisfying input to this
computer program" it's a very <em>structured</em> big problem, and we
have mathematical tools to "compress" it.</p>
<h3 id="from-linear-systems-to-polynomials">From linear systems to
polynomials</h3>
<p>If you have read about <a
href="../../../2017/11/09/starks_part_1.html">STARKs</a> or <a
href="https://medium.com/@VitalikButerin/quadratic-arithmetic-programs-from-zero-to-hero-f6d558cea649">QAPs</a>,
the mechanism described in this next section will hopefully feel
somewhat familiar, but if you have not that's okay too. The main
ingredient here is to understand a <em>polynomial</em> as a mathematical
tool for encapsulating a whole lot of values into a single object.
Typically, we think of polynomials in "coefficient form", that is an
expression like:</p>
<p><span class="math display">\[
y=x^{3}-5 x^{2}+7 x-2
\]</span></p>
<p>But we can also view polynomials in "evaluation form". For example,
we can think of the above as being "the" degree <span
class="math inline">\(&lt; 4\)</span> polynomial with evaluations <span
class="math inline">\((-2, 1, 0, 1)\)</span> at the coordinates <span
class="math inline">\((0, 1, 2, 3)\)</span> respectively.</p>
<center>
<img src="../../../../images/plonk-files/polynomial_graph.png" />
</center>
<p><br></p>
<p>Now here's the next step. Systems of many equations of the same form
can be re-interpreted as a single equation over polynomials. For
example, suppose that we have the system:</p>
<p><span class="math display">\[
\begin{array}{l}{2 x_{1}-x_{2}+3 x_{3}=8} \\ {x_{1}+4 x_{2}-5 x_{3}=5}
\\ {8 x_{1}-x_{2}-x_{3}=-2}\end{array}
\]</span></p>
<p>Let us define four polynomials in evaluation form: <span
class="math inline">\(L(x)\)</span> is the degree <span
class="math inline">\(&lt; 3\)</span> polynomial that evaluates to <span
class="math inline">\((2, 1, 8)\)</span> at the coordinates <span
class="math inline">\((0, 1, 2)\)</span>, and at those same coordinates
<span class="math inline">\(M(x)\)</span> evaluates to <span
class="math inline">\((-1, 4, -1)\)</span>, <span
class="math inline">\(R(x)\)</span> to <span class="math inline">\((3,
-5, -1)\)</span> and <span class="math inline">\(O(x)\)</span> to <span
class="math inline">\((8, 5, -2)\)</span> (it is okay to directly define
polynomials in this way; you can use <a
href="https://en.wikipedia.org/wiki/Lagrange_interpolation">Lagrange
interpolation</a> to convert to coefficient form). Now, consider the
equation:</p>
<p><span class="math display">\[
L(x) \cdot x_{1}+M(x) \cdot x_{2}+R(x) \cdot x_{3}-O(x)=Z(x) H(x)
\]</span></p>
<p>Here, <span class="math inline">\(Z(x)\)</span> is shorthand for
<span class="math inline">\((x-0) \cdot (x-1) \cdot (x-2)\)</span> - the
minimal (nontrivial) polynomial that returns zero over the evaluation
domain <span class="math inline">\((0, 1, 2)\)</span>. A solution to
this equation (<span class="math inline">\(x_1 = 1, x_2 = 6, x_3 = 4,
H(x) = 0\)</span>) is also a solution to the original system of
equations, except the original system does not need <span
class="math inline">\(H(x)\)</span>. Notice also that in this case,
<span class="math inline">\(H(x)\)</span> is conveniently zero, but in
more complex cases <span class="math inline">\(H\)</span> may need to be
nonzero.</p>
<p>So now we know that we can represent a large set of constraints
within a small number of mathematical objects (the polynomials). But in
the equations that we made above to represent the gate wire constraints,
the <span class="math inline">\(x_1, x_2, x_3\)</span> variables are
different per equation. We can handle this by making the variables
themselves polynomials rather than constants in the same way. And so we
get:</p>
<p><span class="math display">\[
Q_{L}(x) a(x)+Q_{R}(x) b(x)+Q_{O}(x) c(x)+Q_{M}(x) a(x) b(x)+Q_{C}(x)=0
\]</span></p>
<p>As before, each <span class="math inline">\(Q\)</span> polynomial is
a parameter that can be generated from the program that is being
verified, and the <span class="math inline">\(a\)</span>, <span
class="math inline">\(b\)</span>, <span class="math inline">\(c\)</span>
polynomials are the user-provided inputs.</p>
<h3 id="copy-constraints">Copy constraints</h3>
<p>Now, let us get back to "connecting" the wires. So far, all we have
is a bunch of disjoint equations about disjoint values that are
independently easy to satisfy: constant gates can be satisfied by
setting the value to the constant and addition and multiplication gates
can simply be satisfied by setting all wires to zero! To make the
problem actually challenging (and actually represent the problem encoded
in the original circuit), we need to add an equation that verifies "copy
constraints": constraints such as <span class="math inline">\(a(5) =
c(7)\)</span>, <span class="math inline">\(c(10) = c(12)\)</span>, etc.
This requires some clever trickery.</p>
<p>Our strategy will be to design a "coordinate pair accumulator", a
polynomial <span class="math inline">\(p(x)\)</span> which works as
follows. First, let <span class="math inline">\(X(x)\)</span> and <span
class="math inline">\(Y(x)\)</span> be two polynomials representing the
<span class="math inline">\(x\)</span> and <span
class="math inline">\(y\)</span> coordinates of a set of points (eg. to
represent the set <span class="math inline">\(((0, -2), (1, 1), (2, 0),
(3, 1))\)</span> you might set <span class="math inline">\(X(x) =
x\)</span> and <span class="math inline">\(Y(x) = x^3 - 5x^2 + 7x -
2)\)</span>. Our goal will be to let <span
class="math inline">\(p(x)\)</span> represent all the points up to (but
not including) the given position, so <span
class="math inline">\(p(0)\)</span> starts at <span
class="math inline">\(1\)</span>, <span
class="math inline">\(p(1)\)</span> represents just the first point,
<span class="math inline">\(p(2)\)</span> the first and the second, etc.
We will do this by "randomly" selecting two constants, <span
class="math inline">\(v_1\)</span> and <span
class="math inline">\(v_2\)</span>, and constructing <span
class="math inline">\(p(x)\)</span> using the constraints <span
class="math inline">\(p(0) = 1\)</span> and <span
class="math inline">\(p(x+1) = p(x) \cdot (v_1 + X(x) + v_2 \cdot
Y(x))\)</span> at least within the domain <span
class="math inline">\((0, 1, 2, 3)\)</span>.</p>
<p>For example, letting <span class="math inline">\(v_1 = 3\)</span> and
<span class="math inline">\(v_2 = 2\)</span>, we get:</p>
<center>
<p><img src="../../../../images/plonk-files/polynomial_graph3.png" style="width:440px"/><br></p>
<table style="padding-right:136px; border-collapse: collapse;" align="center">
<tr>
<td style="border: 1px solid black" align="right" width="136px">
X(x)
</td>
<td style="border: 1px solid black" align="center" width="84px">
0
</td>
<td style="border: 1px solid black" align="center" width="84px">
1
</td>
<td style="border: 1px solid black" align="center" width="84px">
2
</td>
<td style="border: 1px solid black" align="center" width="84px">
3
</td>
<td style="border: 1px solid black" align="center" width="84px">
4
</td>
</tr>
<tr>
<td style="border: 1px solid black" align="right" width="136px">
<span class="math inline">\(Y(x)\)</span>
</td>
<td style="border: 1px solid black" align="center" width="84px">
-2
</td>
<td style="border: 1px solid black" align="center" width="84px">
1
</td>
<td style="border: 1px solid black" align="center" width="84px">
0
</td>
<td style="border: 1px solid black" align="center" width="84px">
1
</td>
<td style="border: 1px solid black" align="center" width="84px">
</td>
</tr>
<tr>
<td style="border: 1px solid black" align="right" width="136px">
<small><span class="math inline">\(v_1 + X(x) + v_2 \cdot
Y(x)\)</span></small>
</td>
<td style="border: 1px solid black" align="center" width="84px">
-1
</td>
<td style="border: 1px solid black" align="center" width="84px">
6
</td>
<td style="border: 1px solid black" align="center" width="84px">
5
</td>
<td style="border: 1px solid black" align="center" width="84px">
8
</td>
<td style="border: 1px solid black" align="center" width="84px">
</td>
</tr>
<tr>
<td style="border: 1px solid black" align="right" width="136px">
<span class="math inline">\(p(x)\)</span>
</td>
<td style="border: 1px solid black" align="center" width="84px">
1
</td>
<td style="border: 1px solid black" align="center" width="84px">
-1
</td>
<td style="border: 1px solid black" align="center" width="84px">
-6
</td>
<td style="border: 1px solid black" align="center" width="84px">
-30
</td>
<td style="border: 1px solid black" align="center" width="84px">
-240
</td>
</tr>
</table>
<br> <small><i>Notice that (aside from the first column) every <span
class="math inline">\(p(x)\)</span> value equals the value to the left
of it multiplied by the value to the left and above it.</i></small>
</center>
<p><br></p>
<p>The result we care about is <span class="math inline">\(p(4) =
-240\)</span>. Now, consider the case where instead of <span
class="math inline">\(X(x) = x\)</span>, we set <span
class="math inline">\(X(x) = \frac{2}{3} x^3 - 4x^2 +
\frac{19}{3}x\)</span> (that is, the polynomial that evaluates to <span
class="math inline">\((0, 3, 2, 1)\)</span> at the coordinates <span
class="math inline">\((0, 1, 2, 3)\)</span>). If you run the same
procedure, you'll find that you also get <span
class="math inline">\(p(4) = -240\)</span>. This is not a coincidence
(in fact, if you randomly pick <span class="math inline">\(v_1\)</span>
and <span class="math inline">\(v_2\)</span> from a sufficiently large
field, it will <em>almost never</em> happen coincidentally). Rather,
this happens because <span class="math inline">\(Y(1) = Y(3)\)</span>,
so if you "swap the <span class="math inline">\(X\)</span> coordinates"
of the points <span class="math inline">\((1, 1)\)</span> and <span
class="math inline">\((3, 1)\)</span> you're not changing the
<em>set</em> of points, and because the accumulator encodes a set (as
multiplication does not care about order) the value at the end will be
the same.</p>
<p>Now we can start to see the basic technique that we will use to prove
copy constraints. First, consider the simple case where we only want to
prove copy constraints within one set of wires (eg. we want to prove
<span class="math inline">\(a(1) = a(3)\)</span>). We'll make two
coordinate accumulators: one where <span class="math inline">\(X(x) =
x\)</span> and <span class="math inline">\(Y(x) = a(x)\)</span>, and the
other where <span class="math inline">\(Y(x) = a(x)\)</span> but <span
class="math inline">\(X&#39;(x)\)</span> is the polynomial that
evaluates to the permutation that flips (or otherwise rearranges) the
values in each copy constraint; in the <span class="math inline">\(a(1)
= a(3)\)</span> case this would mean the permutation would start <span
class="math inline">\(0 3 2 1 4...\)</span>. The first accumulator would
be compressing <span class="math inline">\(((0, a(0)), (1, a(1)), (2,
a(2)), (3, a(3)), (4, a(4))...\)</span>, the second <span
class="math inline">\(((0, a(0)), (3, a(1)), (2, a(2)), (1, a(3)), (4,
a(4))...\)</span>. The only way the two can give the same result is if
<span class="math inline">\(a(1) = a(3)\)</span>.</p>
<p>To prove constraints between <span class="math inline">\(a\)</span>,
<span class="math inline">\(b\)</span> and <span
class="math inline">\(c\)</span>, we use the same procedure, but instead
"accumulate" together points from all three polynomials. We assign each
of <span class="math inline">\(a\)</span>, <span
class="math inline">\(b\)</span>, <span class="math inline">\(c\)</span>
a range of <span class="math inline">\(X\)</span> coordinates (eg. <span
class="math inline">\(a\)</span> gets <span class="math inline">\(X_a(x)
= x\)</span> ie. <span class="math inline">\(0...n-1\)</span>, <span
class="math inline">\(b\)</span> gets <span class="math inline">\(X_b(x)
= n+x\)</span>, ie. <span class="math inline">\(n...2n-1\)</span>, <span
class="math inline">\(c\)</span> gets <span class="math inline">\(X_c(x)
= 2n+x\)</span>, ie. <span class="math inline">\(2n...3n-1\)</span>. To
prove copy constraints that hop between different sets of wires, the
"alternate" <span class="math inline">\(X\)</span> coordinates would be
slices of a permutation across all three sets. For example, if we want
to prove <span class="math inline">\(a(2) = b(4)\)</span> with <span
class="math inline">\(n = 5\)</span>, then <span
class="math inline">\(X&#39;_a(x)\)</span> would have evaluations <span
class="math inline">\(\{0, 1, 9, 3, 4\}\)</span> and <span
class="math inline">\(X&#39;_b(x)\)</span> would have evaluations <span
class="math inline">\(\{5, 6, 7, 8, 2\}\)</span> (notice the <span
class="math inline">\(2\)</span> and <span
class="math inline">\(9\)</span> flipped, where <span
class="math inline">\(9\)</span> corresponds to the <span
class="math inline">\(b_4\)</span> wire). Often, <span
class="math inline">\(X&#39;_a(x)\)</span>, <span
class="math inline">\(X&#39;_b(x)\)</span> and <span
class="math inline">\(X&#39;_c(x)\)</span> are also called <span
class="math inline">\(\sigma_a(x)\)</span>, <span
class="math inline">\(\sigma_b(x)\)</span> and <span
class="math inline">\(\sigma_c(x)\)</span>.</p>
<p>We would then instead of checking equality within one run of the
procedure (ie. checking <span class="math inline">\(p(4) =
p&#39;(4)\)</span> as before), we would check <em>the product</em> of
the three different runs on each side:</p>
<p><span class="math display">\[
p_{a}(n) \cdot p_{b}(n) \cdot p_{c}(n)=p_{a}^{\prime}(n) \cdot
p_{b}^{\prime}(n) \cdot p_{c}^{\prime}(n)
\]</span></p>
<p>The product of the three <span class="math inline">\(p(n)\)</span>
evaluations on each side accumulates <em>all</em> coordinate pairs in
the <span class="math inline">\(a\)</span>, <span
class="math inline">\(b\)</span> and <span
class="math inline">\(c\)</span> runs on each side together, so this
allows us to do the same check as before, except that we can now check
copy constraints not just between positions within one of the three sets
of wires <span class="math inline">\(a\)</span>, <span
class="math inline">\(b\)</span> or <span
class="math inline">\(c\)</span>, but also between one set of wires and
another (eg. as in <span class="math inline">\(a(2) =
b(4)\)</span>).</p>
<p>And that's all there is to it!</p>
<h3 id="putting-it-all-together">Putting it all together</h3>
<p>In reality, all of this math is done not over integers, but over a
prime field; check the section "A Modular Math Interlude" <a
href="../../../2017/11/22/starks_part_2.html">here</a> for a description
of what prime fields are. Also, for mathematical reasons perhaps best
appreciated by reading and understanding <a
href="../../../2019/05/12/fft.html">this article on FFT
implementation</a>, instead of representing wire indices with <span
class="math inline">\(x=0....n-1\)</span>, we'll use powers of <span
class="math inline">\(\omega: 1, \omega, \omega ^2....\omega
^{n-1}\)</span> where <span class="math inline">\(\omega\)</span> is a
high-order root-of-unity in the field. This changes nothing about the
math, except that the coordinate pair accumulator constraint checking
equation changes from <span class="math inline">\(p(x + 1) = p(x) \cdot
(v_1 + X(x) + v_2 \cdot Y(x))\)</span> to <span
class="math inline">\(p(\omega \cdot x) = p(x) \cdot (v_1 + X(x) + v_2
\cdot Y(x))\)</span>, and instead of using <span
class="math inline">\(0..n-1\)</span>, <span
class="math inline">\(n..2n-1\)</span>, <span
class="math inline">\(2n..3n-1\)</span> as coordinates we use <span
class="math inline">\(\omega^i, g \cdot \omega^i\)</span> and <span
class="math inline">\(g^2 \cdot \omega^i\)</span> where <span
class="math inline">\(g\)</span> can be some random high-order element
in the field.</p>
<p>Now let's write out all the equations we need to check. First, the
main gate-constraint satisfaction check:</p>
<p><span class="math display">\[
Q_{L}(x) a(x)+Q_{R}(x) b(x)+Q_{O}(x) c(x)+Q_{M}(x) a(x) b(x)+Q_{C}(x)=0
\]</span></p>
<p>Then the polynomial accumulator transition constraint (note: think of
"<span class="math inline">\(= Z(x) \cdot H(x)\)</span>" as meaning
"equals zero for all coordinates within some particular domain that we
care about, but not necessarily outside of it"):</p>
<p><span class="math display">\[
\begin{array}{l}{P_{a}(\omega x)-P_{a}(x)\left(v_{1}+x+v_{2} a(x)\right)
=Z(x) H_{1}(x)} \\ {P_{a^{\prime}}(\omega
x)-P_{a^{\prime}}(x)\left(v_{1}+\sigma_{a}(x)+v_{2} a(x)\right)=Z(x)
H_{2}(x)} \\ {P_{b}(\omega x)-P_{b}(x)\left(v_{1}+g x+v_{2}
b(x)\right)=Z(x) H_{3}(x)} \\ {P_{b^{\prime}}(\omega
x)-P_{b^{\prime}}(x)\left(v_{1}+\sigma_{b}(x)+v_{2} b(x)\right)=Z(x)
H_{4}(x)} \\ {P_{c}(\omega x)-P_{c}(x)\left(v_{1}+g^{2} x+v_{2}
c(x)\right)=Z(x) H_{5}(x)} \\ {P_{c^{\prime}}(\omega
x)-P_{c^{\prime}}(x)\left(v_{1}+\sigma_{c}(x)+v_{2} c(x)\right)=Z(x)
H_{6}(x)}\end{array}
\]</span></p>
<p>Then the polynomial accumulator starting and ending constraints:</p>
<p><span class="math display">\[
\begin{array}{l}{P_{a}(1)=P_{b}(1)=P_{c}(1)=P_{a^{\prime}}(1)=P_{b^{\prime}}(1)=P_{c^{\prime}}(1)=1}
\\ {P_{a}\left(\omega^{n}\right) P_{b}\left(\omega^{n}\right)
P_{c}\left(\omega^{n}\right)=P_{a^{\prime}}\left(\omega^{n}\right)
P_{b^{\prime}}\left(\omega^{n}\right)
P_{c^{\prime}}\left(\omega^{n}\right)}\end{array}
\]</span></p>
<p>The user-provided polynomials are:</p>
<ul>
<li>The wire assignments <span class="math inline">\(a(x), b(x),
c(x)\)</span></li>
<li>The coordinate accumulators <span class="math inline">\(P_a(x),
P_b(x), P_c(x), P_{a&#39;}(x), P_{b&#39;}(x),
P_{c&#39;}(x)\)</span></li>
<li>The quotients <span class="math inline">\(H(x)\)</span> and <span
class="math inline">\(H_1(x)...H_6(x)\)</span></li>
</ul>
<p>The program-specific polynomials that the prover and verifier need to
compute ahead of time are:</p>
<ul>
<li><span class="math inline">\(Q_L(x), Q_R(x), Q_O(x), Q_M(x),
Q_C(x)\)</span>, which together represent the gates in the circuit (note
that <span class="math inline">\(Q_C(x)\)</span> encodes public inputs,
so it may need to be computed or modified at runtime)</li>
<li>The "permutation polynomials" <span
class="math inline">\(\sigma_a(x), \sigma_b(x)\)</span> and <span
class="math inline">\(\sigma_c(x)\)</span>, which encode the copy
constraints between the <span class="math inline">\(a\)</span>, <span
class="math inline">\(b\)</span> and <span
class="math inline">\(c\)</span> wires</li>
</ul>
<p>Note that the verifier need only store commitments to these
polynomials. The only remaining polynomial in the above equations is
<span class="math inline">\(Z(x) = (x - 1) \cdot (x - \omega) \cdot ...
\cdot (x - \omega ^{n-1})\)</span> which is designed to evaluate to zero
at all those points. Fortunately, <span
class="math inline">\(\omega\)</span> can be chosen to make this
polynomial very easy to evaluate: the usual technique is to choose <span
class="math inline">\(\omega\)</span> to satisfy <span
class="math inline">\(\omega ^n = 1\)</span>, in which case <span
class="math inline">\(Z(x) = x^n - 1\)</span>.</p>
<p>There is one nuance here: the constraint between <span
class="math inline">\(P_a(\omega^{i+1})\)</span> and <span
class="math inline">\(P_a(\omega^i)\)</span> can't be true across the
<em>entire</em> circle of powers of <span
class="math inline">\(\omega\)</span>; it's almost always false at <span
class="math inline">\(\omega^{n-1}\)</span> as the next coordinate is
<span class="math inline">\(\omega^n = 1\)</span> which brings us back
to the <em>start</em> of the "accumulator"; to fix this, we can modify
the constraint to say "<em>either</em> the constraint is true
<em>or</em> <span class="math inline">\(x = \omega^{n-1}\)</span>",
which one can do by multiplying <span class="math inline">\(x -
\omega^{n-1}\)</span> into the constraint so it equals zero at that
point.</p>
<p>The only constraint on <span class="math inline">\(v_1\)</span> and
<span class="math inline">\(v_2\)</span> is that the user must not be
able to choose <span class="math inline">\(a(x), b(x)\)</span> or <span
class="math inline">\(c(x)\)</span> after <span
class="math inline">\(v_1\)</span> and <span
class="math inline">\(v_2\)</span> become known, so we can satisfy this
by computing <span class="math inline">\(v_1\)</span> and <span
class="math inline">\(v_2\)</span> from hashes of commitments to <span
class="math inline">\(a(x), b(x)\)</span> and <span
class="math inline">\(c(x)\)</span>.</p>
<p>So now we've turned the program satisfaction problem into a simple
problem of satisfying a few equations with polynomials, and there are
some optimizations in PLONK that allow us to remove many of the
polynomials in the above equations that I will not go into to preserve
simplicity. But the polynomials themselves, both the program-specific
parameters and the user inputs, are <strong>big</strong>. So the next
question is, how do we get around this so we can make the proof
short?</p>
<h2 id="polynomial-commitments">Polynomial commitments</h2>
<p>A <a
href="https://pdfs.semanticscholar.org/31eb/add7a0109a584cfbf94b3afaa3c117c78c91.pdf">polynomial
commitment</a> is a short object that "represents" a polynomial, and
allows you to verify evaluations of that polynomial, without needing to
actually contain all of the data in the polynomial. That is, if someone
gives you a commitment <span class="math inline">\(c\)</span>
representing <span class="math inline">\(P(x)\)</span>, they can give
you a proof that can convince you, for some specific <span
class="math inline">\(z\)</span>, what the value of <span
class="math inline">\(P(z)\)</span> is. There is a further mathematical
result that says that, over a sufficiently big field, if certain kinds
of equations (chosen before <span class="math inline">\(z\)</span> is
known) about polynomials evaluated at a random <span
class="math inline">\(z\)</span> are true, those same equations are true
about the whole polynomial as well. For example, if <span
class="math inline">\(P(z) \cdot Q(z) + R(z) = S(z) + 5\)</span>, then
we know that it's overwhelmingly likely that <span
class="math inline">\(P(x) \cdot Q(x) + R(x) = S(x) + 5\)</span> in
general. Using such polynomial commitments, we could very easily check
all of the above polynomial equations above - make the commitments, use
them as input to generate <span class="math inline">\(z\)</span>, prove
what the evaluations are of each polynomial at <span
class="math inline">\(z\)</span>, and then run the equations with these
evaluations instead of the original polynomials. But how do these
commitments work?</p>
<p>There are two parts: the commitment to the polynomial <span
class="math inline">\(P(x) \rightarrow c\)</span>, and the opening to a
value <span class="math inline">\(P(z)\)</span> at some <span
class="math inline">\(z\)</span>. To make a commitment, there are many
techniques; one example is <a
href="../../../2017/11/22/starks_part_2.html">FRI</a>, and another is
Kate commitments which I will describe below. To prove an opening, it
turns out that there is a simple generic "subtract-and-divide" trick: to
prove that <span class="math inline">\(P(z) = a\)</span>, you prove
that</p>
<p><span class="math display">\[
\frac{P(x)-a}{x-z}
\]</span></p>
<p>is also a polynomial (using another polynomial commitment). This
works because if the quotient is a polynomial (ie. it is not
fractional), then <span class="math inline">\(x - z\)</span> is a factor
of <span class="math inline">\(P(x) - a\)</span>, so <span
class="math inline">\((P(x) - a)(z) = 0\)</span>, so <span
class="math inline">\(P(z) = a\)</span>. Try it with some polynomial,
eg. <span class="math inline">\(P(x) = x^3 + 2 \cdot x^2 + 5\)</span>
with <span class="math inline">\((z = 6, a = 293)\)</span>, yourself;
and try <span class="math inline">\((z = 6, a = 292)\)</span> and see
how it fails (if you're lazy, see WolframAlpha <a
href="https://www.wolframalpha.com/input/?i=factor+%28%28x%5E3+%2B+2*x%5E2+%2B+5%29+-+293%29+%2F+%28x+-+6%29">here</a>
vs <a
href="https://www.wolframalpha.com/input/?i=factor+%28%28x%5E3+%2B+2*x%5E2+%2B+5%29+-+292%29+%2F+%28x+-+6%29">here</a>).
Note also a generic optimization: to prove many openings of many
polynomials at the same time, after committing to the outputs do the
subtract-and-divide trick on a <em>random linear combination</em> of the
polynomials and the outputs.</p>
<p>So how do the commitments themselves work? Kate commitments are,
fortunately, much simpler than FRI. A trusted-setup procedure generates
a set of elliptic curve points <span class="math inline">\(G, G \cdot s,
G \cdot s^2\)</span> .... <span class="math inline">\(G \cdot
s^n\)</span>, as well as <span class="math inline">\(G_2 \cdot
s\)</span>, where <span class="math inline">\(G\)</span> and <span
class="math inline">\(G_2\)</span> are the generators of two elliptic
curve groups and <span class="math inline">\(s\)</span> is a secret that
is forgotten once the procedure is finished (note that there is a
multi-party version of this setup, which is secure as long as at least
one of the participants forgets their share of the secret). These points
are published and considered to be "the proving key" of the scheme;
anyone who needs to make a polynomial commitment will need to use these
points. A commitment to a degree-d polynomial is made by multiplying
each of the first d+1 points in the proving key by the corresponding
coefficient in the polynomial, and adding the results together.</p>
<p>Notice that this provides an "evaluation" of that polynomial at <span
class="math inline">\(s\)</span>, without knowing <span
class="math inline">\(s\)</span>. For example, <span
class="math inline">\(x^3 + 2x^2+5\)</span> would be represented by
<span class="math inline">\((G \cdot s^3) + 2 \cdot (G \cdot s^2) + 5
\cdot G\)</span>. We can use the notation <span
class="math inline">\([P]\)</span> to refer to <span
class="math inline">\(P\)</span> encoded in this way (ie. <span
class="math inline">\(G \cdot P(s)\)</span>). When doing the
subtract-and-divide trick, you can prove that the two polynomials
actually satisfy the relation by using <a
href="https://medium.com/@VitalikButerin/exploring-elliptic-curve-pairings-c73c1864e627">elliptic
curve pairings</a>: check that <span class="math inline">\(e([P] - G
\cdot a, G_2) = e([Q], [x] - G_2 \cdot z)\)</span> as a proxy for
checking that <span class="math inline">\(P(x) - a = Q(x) \cdot (x -
z)\)</span>.</p>
<p>But there are more recently other types of polynomial commitments
coming out too. A new scheme called DARK ("Diophantine arguments of
knowledge") uses "hidden order groups" such as <a
href="https://blogs.ams.org/mathgradblog/2018/02/10/introduction-ideal-class-groups/">class
groups</a> to implement another kind of polynomial commitment. Hidden
order groups are unique because they allow you to compress arbitrarily
large numbers into group elements, even numbers much larger than the
size of the group element, in a way that can't be "spoofed";
constructions from VDFs to <a
href="https://ethresear.ch/t/rsa-accumulators-for-plasma-cash-history-reduction/3739">accumulators</a>
to range proofs to polynomial commitments can be built on top of this.
Another option is to use bulletproofs, using regular elliptic curve
groups at the cost of the proof taking much longer to verify. Because
polynomial commitments are much simpler than full-on zero knowledge
proof schemes, we can expect more such schemes to get created in the
future.</p>
<h2 id="recap">Recap</h2>
<p>To finish off, let's go over the scheme again. Given a program <span
class="math inline">\(P\)</span>, you convert it into a circuit, and
generate a set of equations that look like this:</p>
<p><span class="math display">\[
\left(Q_{L_{i}}\right) a_{i}+\left(Q_{R_{i}}\right)
b_{i}+\left(Q_{O_{i}}\right) c_{i}+\left(Q_{M_{i}}\right) a_{i}
b_{i}+Q_{C_{i}}=0
\]</span></p>
<p>You then convert this set of equations into a single polynomial
equation:</p>
<p><span class="math display">\[
Q_{L}(x) a(x)+Q_{R}(x) b(x)+Q_{O}(x) c(x)+Q_{M}(x) a(x) b(x)+Q_{C}(x)=0
\]</span></p>
<p>You also generate from the circuit a list of copy constraints. From
these copy constraints you generate the three polynomials representing
the permuted wire indices: <span class="math inline">\(\sigma_a(x),
\sigma_b(x), \sigma_c(x)\)</span>. To generate a proof, you compute the
values of all the wires and convert them into three polynomials: <span
class="math inline">\(a(x), b(x), c(x)\)</span>. You also compute six
"coordinate pair accumulator" polynomials as part of the
permutation-check argument. Finally you compute the cofactors <span
class="math inline">\(H_i(x)\)</span>.</p>
<p>There is a set of equations between the polynomials that need to be
checked; you can do this by making commitments to the polynomials,
opening them at some random <span class="math inline">\(z\)</span>
(along with proofs that the openings are correct), and running the
equations on these evaluations instead of the original polynomials. The
proof itself is just a few commitments and openings and can be checked
with a few equations. And that's all there is to it!</p>
 </div> 