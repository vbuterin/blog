

<!DOCTYPE html>
<html>
<meta charset="UTF-8">
<style>
@media (prefers-color-scheme: dark) {
    body {
        background-color: #1c1c1c;
        color: white;
    }
    .markdown-body table tr {
        background-color: #1c1c1c;
    }
    .markdown-body table tr:nth-child(2n) {
        background-color: black;
    }
}
</style>



<link rel="alternate" type="application/rss+xml" href="../../../../feed.xml" title="Hard Problems in Cryptocurrency: Five Years Later">



<link rel="stylesheet" type="text/css" href="../../../../css/common-vendor.b8ecfc406ac0b5f77a26.css">
<link rel="stylesheet" type="text/css" href="../../../../css/fretboard.f32f2a8d5293869f0195.css">
<link rel="stylesheet" type="text/css" href="../../../../css/pretty.0ae3265014f89d9850bf.css">
<link rel="stylesheet" type="text/css" href="../../../../css/pretty-vendor.83ac49e057c3eac4fce3.css">
<link rel="stylesheet" type="text/css" href="../../../../css/global.css">
<link rel="stylesheet" type="text/css" href="../../../../css/misc.css">

<script type="text/x-mathjax-config">
<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\(', '\)']]
  },
  svg: {
    fontCache: 'global',
  }
};
</script>
<script type="text/javascript" id="MathJax-script" async
  src="../../../../scripts/tex-svg.js">
</script>

<style>
</style>

<div id="doc" class="container-fluid markdown-body comment-enabled" data-hard-breaks="true">

<div id="color-mode-switch">
  <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2">
    <path stroke-linecap="round" stroke-linejoin="round" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364l-.707-.707M6.343 6.343l-.707-.707m12.728 0l-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z" />
  </svg>
  <input type="checkbox" id="switch" />
  <label for="switch">Dark Mode Toggle</label>
  <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2">
    <path stroke-linecap="round" stroke-linejoin="round" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003 0 0012 21a9.003 9.003 0 008.354-5.646z" />
  </svg>
</div>

<script type="text/javascript">
  // Update root html class to set CSS colors
  const toggleDarkMode = () => {
    const root = document.querySelector('html');
    root.classList.toggle('dark');
  }

  // Update local storage value for colorScheme
  const toggleColorScheme = () => {
    const colorScheme = localStorage.getItem('colorScheme');
    if (colorScheme === 'light') localStorage.setItem('colorScheme', 'dark');
    else localStorage.setItem('colorScheme', 'light');
  }

  // Set toggle input handler
  const toggle = document.querySelector('#color-mode-switch input[type="checkbox"]');
  if (toggle) toggle.onclick = () => {
    toggleDarkMode();
    toggleColorScheme();
  }

  // Check for color scheme on init
  const checkColorScheme = () => {
    const colorScheme = localStorage.getItem('colorScheme');
    // Default to light for first view
    if (colorScheme === null || colorScheme === undefined) localStorage.setItem('colorScheme', 'light');
    // If previously saved to dark, toggle switch and update colors
    if (colorScheme === 'dark') {
      toggle.checked = true;
      toggleDarkMode();
    }
  }
  checkColorScheme();
</script>

<meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="Hard Problems in Cryptocurrency: Five Years Later" />
<meta name="twitter:image" content="http://vitalik.eth.limo/images/icon.png" />


<br>
<h1 style="margin-bottom:7px"> Hard Problems in Cryptocurrency: Five Years Later </h1>
<small style="float:left; color: #888"> 2019 Nov 22 </small>
<small style="float:right; color: #888"><a href="../../../../index.html">See all posts</a></small>
<br> <br> <br>
<title> Hard Problems in Cryptocurrency: Five Years Later </title>

<p><em>Special thanks to Justin Drake and Jinglan Wang for
feedback</em></p>
<p>In 2014, I made a <a
href="https://github.com/ethereum/wiki/wiki/Problems/89fd07ffff8b042134e4ca67a0ce143d574016bd">post</a>
and a <a
href="https://www.youtube.com/watch?v=rXRtJcNVfQE">presentation</a> with
a list of hard problems in math, computer science and economics that I
thought were important for the cryptocurrency space (as I then called
it) to be able to reach maturity. In the last five years, much has
changed. But exactly how much progress on what we thought then was
important has been achieved? Where have we succeeded, where have we
failed, and where have we changed our minds about what is important? In
this post, I'll go through the 16 problems from 2014 one by one, and see
just where we are today on each one. At the end, I'll include my new
picks for hard problems of 2019.</p>
<p>The problems are broken down into three categories: (i)
cryptographic, and hence expected to be solvable with purely
mathematical techniques if they are to be solvable at all, (ii)
consensus theory, largely improvements to proof of work and proof of
stake, and (iii) economic, and hence having to do with creating
structures involving incentives given to different participants, and
often involving the application layer more than the protocol layer. We
see significant progress in all categories, though some more than
others.</p>
<h2 id="cryptographic-problems">Cryptographic problems</h2>
<blockquote style="background-color:#ffe4ff; padding-top: 18px; padding-bottom: 18px">
<h3>
<ol type="1">
<li>Blockchain Scalability
</h3>
One of the largest problems facing the cryptocurrency space today is the
issue of scalability ... The main concern with [oversized blockchains] is
trust: if there are only a few entities capable of running full nodes,
then those entities can conspire and agree to give themselves a large
number of additional bitcoins, and there would be no way for other users
to see for themselves that a block is invalid without processing an
entire block themselves. <strong>Problem:</strong> create a blockchain
design that maintains Bitcoin-like security guarantees, but where the
maximum size of the most powerful node that needs to exist for the
network to keep functioning is substantially sublinear in the number of
transactions.
</blockquote></li>
</ol>
<p>Status: <strong>Great theoretical progress, pending more real-world
evaluation</strong>.
<img src="../../../../images/progress-files/happy_face7.png" style="width:50px; height: 50px" class="transparent" /></p>
<p>Scalability is one technical problem that we have had a huge amount
of progress on theoretically. Five years ago, almost no one was thinking
about sharding; now, sharding designs are commonplace. Aside from <a
href="https://github.com/ethereum/eth2.0-specs">ethereum 2.0</a>, we
have <a href="https://eprint.iacr.org/2017/406.pdf">OmniLedger</a>, <a
href="https://arxiv.org/abs/1905.09274">LazyLedger</a>, <a
href="https://medium.com/@giottodf/zilliqa-a-novel-approach-to-sharding-d79249347a1f">Zilliqa</a>
and research papers <a
href="https://arxiv.org/pdf/1910.10434.pdf">seemingly coming out every
month</a>. In my own view, further progress at this point is
incremental. Fundamentally, we already have a number of techniques that
allow groups of validators to securely come to consensus on much more
data than an individual validator can process, as well as techniques
allow clients to indirectly verify the full validity and availability of
blocks even under 51% attack conditions.</p>
<p>These are probably the most important technologies:</p>
<ul>
<li><strong>Random sampling</strong>, allowing a small randomly selected
committee to statistically stand in for the full validator set: <a
href="https://github.com/ethereum/wiki/wiki/Sharding-FAQ#how-can-we-solve-the-single-shard-takeover-attack-in-an-uncoordinated-majority-model">https://github.com/ethereum/wiki/wiki/Sharding-FAQ#how-can-we-solve-the-single-shard-takeover-attack-in-an-uncoordinated-majority-model</a></li>
<li><strong>Fraud proofs</strong>, allowing individual nodes that learn
of an error to broadcast its presence to everyone else: <a
href="https://bitcoin.stackexchange.com/questions/49647/what-is-a-fraud-proof">https://bitcoin.stackexchange.com/questions/49647/what-is-a-fraud-proof</a></li>
<li><strong>Proofs of custody</strong>, allowing validators to
probabilistically prove that they individually downloaded and verified
some piece of data: <a
href="https://ethresear.ch/t/1-bit-aggregation-friendly-custody-bonds/2236">https://ethresear.ch/t/1-bit-aggregation-friendly-custody-bonds/2236</a></li>
<li><strong>Data availability proofs</strong>, allowing clients to
detect when the bodies of blocks that they have headers for <a
href="https://github.com/ethereum/research/wiki/A-note-on-data-availability-and-erasure-coding">are
unavailable</a>: <a
href="https://arxiv.org/abs/1809.09044">https://arxiv.org/abs/1809.09044</a>.
See also the newer <a href="https://arxiv.org/abs/1910.01247">coded
Merkle trees</a> proposal.</li>
</ul>
<p>There are also other smaller developments like <a
href="https://github.com/ethereum/wiki/wiki/Sharding-FAQ#how-can-we-facilitate-cross-shard-communication">Cross-shard
communication via receipts</a> as well as "constant-factor" enhancements
such as BLS signature aggregation.</p>
<p>That said, fully sharded blockchains have still not been seen in live
operation (the partially sharded Zilliqa has recently started running).
On the theoretical side, there are mainly disputes about details
remaining, along with challenges having to do with stability of sharded
networking, developer experience and mitigating risks of centralization;
fundamental technical possibility no longer seems in doubt. But the
challenges that <em>do</em> remain are challenges that cannot be solved
by just thinking about them; only developing the system and seeing
ethereum 2.0 or some similar chain running live will suffice.</p>
<blockquote style="background-color:#ffe4ff; padding-top: 18px; padding-bottom: 18px">
<h3>
<ol start="2" type="1">
<li>Timestamping
</h3>
<strong>Problem:</strong> create a distributed incentive-compatible
system, whether it is an overlay on top of a blockchain or its own
blockchain, which maintains the current time to high accuracy. All
legitimate users have clocks in a normal distribution around some "real"
time with standard deviation 20 seconds ... no two nodes are more than 20
seconds apart The solution is allowed to rely on an existing concept of
"N nodes"; this would in practice be enforced with proof-of-stake or
non-sybil tokens (see #9). The system should continuously provide a time
which is within 120s (or less if possible) of the internal clock of
&gt;99% of honestly participating nodes. External systems may end up
relying on this system; hence, it should remain secure against attackers
controlling &lt; 25% of nodes regardless of incentives.
</blockquote></li>
</ol>
<p>Status: <strong>Some progress</strong>.
<img src="../../../../images/progress-files/happy_face2.png" style="width:50px; height: 50px" class="transparent" /></p>
<p>Ethereum has actually survived just fine with a 13-second block time
and no particularly advanced timestamping technology; it uses a simple
technique where a client does not accept a block whose stated timestamp
is earlier than the client's local time. That said, this has not been
tested under serious attacks. The recent <a
href="https://ethresear.ch/t/network-adjusted-timestamps/4187">network-adjusted
timestamps</a> proposal tries to improve on the status quo by allowing
the client to determine the consensus on the time in the case where the
client does not locally know the current time to high accuracy; this has
not yet been tested. But in general, timestamping is not currently at
the foreground of perceived research challenges; perhaps this will
change once more proof of stake chains (including Ethereum 2.0 but also
others) come online as real live systems and we see what the issues
are.</p>
<blockquote style="background-color:#ffe4ff; padding-top: 18px; padding-bottom: 18px">
<h3>
<ol start="3" type="1">
<li>Arbitrary Proof of Computation
</h3>
<strong>Problem:</strong> create programs
<code>POC_PROVE(P,I) -&gt; (O,Q)</code> and
<code>POC_VERIFY(P,O,Q) -&gt; { 0, 1 }</code> such that
<code>POC_PROVE</code> runs program <code>P</code> on input
<code>I</code> and returns the program output <code>O</code> and a
proof-of-computation <code>Q</code> and POC_VERIFY takes <code>P</code>,
<code>O</code> and <code>Q</code> and outputs whether or not
<code>Q</code> and <code>O</code> were legitimately produced by the
<code>POC_PROVE</code> algorithm using <code>P</code>.
</blockquote></li>
</ol>
<p>Status: <strong>Great theoretical and practical progress</strong>.
<img src="../../../../images/progress-files/happy_face1.png" style="width:50px; height: 50px" class="transparent" /></p>
<p>This is basically saying, build a SNARK (or STARK, or SHARK, or...).
And <a
href="https://medium.com/@VitalikButerin/zk-snarks-under-the-hood-b33151a013f6">we've</a>
<a href="../../../2018/07/21/starks_part_3.html">done</a> <a
href="../../../2019/09/22/plonk.html">it</a>! SNARKs are now
increasingly well understood, and are even already being used in
multiple blockchains today (including <a
href="https://tornado.cash/">tornado.cash</a> on Ethereum). And SNARKs
are extremely useful, both as a privacy technology (see Zcash and
tornado.cash) and as a scalability technology (see <a
href="https://ethresear.ch/t/on-chain-scaling-to-potentially-500-tx-sec-through-mass-tx-validation/3477">ZK
Rollup</a>, <a href="https://www.starkdex.io/">STARKDEX</a> and <a
href="https://ethresear.ch/t/stark-proving-low-degree-ness-of-a-data-availability-root-some-analysis/6214">STARKing
erasure coded data roots</a>).</p>
<p>There are still challenges with efficiency; making
arithmetization-friendly hash functions (see <a
href="https://starkware.co/hash-challenge/">here</a> and <a
href="https://mimchash.org/">here</a> for bounties for breaking proposed
candidates) is a big one, and efficiently proving random memory accesses
is another. Furthermore, there's the unsolved question of whether the
O(n * log(n)) blowup in prover time is a fundamental limitation or if
there is some way to make a succinct proof with only linear overhead as
in <a
href="https://web.stanford.edu/~buenz/pubs/bulletproofs.pdf">bulletproofs</a>
(which unfortunately take linear time to verify). There are also
ever-present risks that the existing schemes have bugs. In general, the
problems are in the details rather than the fundamentals.</p>
<p><a name="numberfour"></a></p>
<blockquote style="background-color:#ffe4ff; padding-top: 18px; padding-bottom: 18px">
<h3>
<ol start="4" type="1">
<li>Code Obfuscation
</h3>
The holy grail is to create an obfuscator O, such that given any program
P the obfuscator can produce a second program O(P) = Q such that P and Q
return the same output if given the same input and, importantly, Q
reveals no information whatsoever about the internals of P. One can hide
inside of Q a password, a secret encryption key, or one can simply use Q
to hide the proprietary workings of the algorithm itself.
</blockquote></li>
</ol>
<p>Status: <strong>Slow progress</strong>.
<img src="../../../../images/progress-files/happy_face3.png" style="width:50px; height: 50px" class="transparent" /></p>
<p>In plain English, the problem is saying that we want to come up with
a way to "encrypt" a program so that the encrypted program would still
give the same outputs for the same inputs, but the "internals" of the
program would be hidden. An example use case for obfuscation is a
program containing a private key where the program only allows the
private key to sign certain messages.</p>
<p>A solution to code obfuscation would be very useful to blockchain
protocols. The use cases are subtle, because one must deal with the
possibility that an on-chain obfuscated program will be copied and run
in an environment different from the chain itself, but there are many
possibilities. One that personally interests me is the ability to remove
the centralized operator from <a
href="https://ethresear.ch/t/minimal-anti-collusion-infrastructure/5413">collusion-resistance
gadgets</a> by replacing the operator with an obfuscated program that
contains some proof of work, making it very expensive to run more than
once with different inputs as part of an attempt to determine individual
participants' actions.</p>
<p>Unfortunately this continues to be a hard problem. There is
continuing ongoing work in attacking the problem, one side making
constructions (eg. <a href="https://eprint.iacr.org/2018/615">this</a>)
that try to reduce the number of assumptions on mathematical objects
that we do not know practically exist (eg. general cryptographic
multilinear maps) and another side trying to make practical
implementations of the desired mathematical objects. However, all of
these paths are still quite far from creating something viable and known
to be secure. See <a
href="https://eprint.iacr.org/2019/463.pdf">https://eprint.iacr.org/2019/463.pdf</a>
for a more general overview to the problem.</p>
<blockquote style="background-color:#ffe4ff; padding-top: 18px; padding-bottom: 18px">
<h3>
<ol start="5" type="1">
<li>Hash-Based Cryptography
</h3>
<strong>Problem:</strong> create a signature algorithm relying on no
security assumption but the random oracle property of hashes that
maintains 160 bits of security against classical computers (ie. 80
vs. quantum due to Grover's algorithm) with optimal size and other
properties.
</blockquote></li>
</ol>
<p>Status: <strong>Some progress</strong>.
<img src="../../../../images/progress-files/happy_face2.png" style="width:50px; height: 50px" class="transparent" /></p>
<p>There have been two strands of progress on this since 2014. <a
href="https://cryptojedi.org/papers/sphincs-20141001.pdf">SPHINCS</a>, a
"stateless" (meaning, using it multiple times does not require
remembering information like a nonce) signature scheme, was released
soon after this "hard problems" list was published, and provides a
purely hash-based signature scheme of size around 41 kB. Additionally,
<a href="../../../2018/07/21/starks_part_3.html">STARKs</a> have been
developed, and one can create signatures of similar size based on them.
The fact that not just signatures, but also general-purpose zero
knowledge proofs, are possible with just hashes was definitely something
I did not expect five years ago; I am very happy that this is the case.
That said, size continues to be an issue, and ongoing progress (eg. see
the very recent <a href="https://arxiv.org/abs/1903.12243">DEEP FRI</a>)
is continuing to reduce the size of proofs, though it looks like further
progress will be incremental.</p>
<p>The main not-yet-solved problem with hash-based cryptography is
aggregate signatures, similar to what <a
href="https://ethresear.ch/t/pragmatic-signature-aggregation-with-bls/2105">BLS
aggregation</a> makes possible. It's known that we can just make a STARK
over many Lamport signatures, but this is inefficient; a more efficient
scheme would be welcome. (In case you're wondering if hash-based
<em>public key encryption</em> is possible, the answer is, no, you can't
do anything with <a
href="https://www.boazbarak.org/Papers/merkle.pdf">more than a quadratic
attack cost</a>)</p>
<h2 id="consensus-theory-problems">Consensus theory problems</h2>
<blockquote style="background-color:#ffe4ff; padding-top: 18px; padding-bottom: 18px">
<h3>
<ol start="6" type="1">
<li>ASIC-Resistant Proof of Work
</h3>
One approach at solving the problem is creating a proof-of-work
algorithm based on a type of computation that is very difficult to
specialize ... For a more in-depth discussion on ASIC-resistant hardware,
see <a
href="https://blog.ethereum.org/2014/06/19/mining/">https://blog.ethereum.org/2014/06/19/mining/</a>.
</blockquote></li>
</ol>
<p>Status: <strong>Solved as far as we can</strong>.
<img src="../../../../images/progress-files/happy_face4.png" style="width:50px; height: 50px" class="transparent" /></p>
<p>About six months after the "hard problems" list was posted, Ethereum
settled on its ASIC-resistant proof of work algorithm: <a
href="https://github.com/ethereum/wiki/wiki/Ethash">Ethash</a>. Ethash
is known as a memory-hard algorithm. The theory is that random-access
memory in regular computers is well-optimized already and hence
difficult to improve on for specialized applications. Ethash aims to
achieve ASIC resistance by making memory access the dominant part of
running the PoW computation. Ethash was not the first memory-hard
algorithm, but it did add one innovation: it uses pseudorandom lookups
over a two-level DAG, allowing for two ways of evaluating the function.
First, one could compute it quickly if one has the entire (~2 GB) DAG;
this is the memory-hard "fast path". Second, one can compute it much
more slowly (still fast enough to check a single provided solution
quickly) if one only has the top level of the DAG; this is used for
block verification.</p>
<p>Ethash has proven remarkably successful at ASIC resistance; after
three years and billions of dollars of block rewards, ASICs do exist but
are at best <a
href="https://blog.miningstore.com/blog/ethereum-mining-hardware-for-2019">2-5
times more power and cost-efficient</a> than GPUs. <a
href="https://github.com/ifdefelse/ProgPOW">ProgPoW</a> has been
proposed as an alternative, but there is a growing consensus that
ASIC-resistant algorithms will inevitably have a limited lifespan, and
that ASIC resistance <a
href="https://pdaian.com/blog/anti-asic-forks-considered-harmful/">has
downsides</a> because it makes 51% attacks cheaper (eg. see the <a
href="https://cointelegraph.com/news/ethereum-classic-51-attack-the-reality-of-proof-of-work">51%
attack on Ethereum Classic</a>).</p>
<p>I believe that PoW algorithms that provide a medium level of ASIC
resistance can be created, but such resistance is limited-term and both
ASIC and non-ASIC PoW have disadvantages; in the long term the better
choice for blockchain consensus is proof of stake.</p>
<blockquote style="background-color:#ffeeff">
<h3>
<ol start="7" type="1">
<li>Useful Proof of Work
</h3>
making the proof of work function something which is simultaneously
useful; a common candidate is something like Folding@home, an existing
program where users can download software onto their computers to
simulate protein folding and provide researchers with a large supply of
data to help them cure diseases.
</blockquote></li>
</ol>
<p>Status: <strong>Probably not feasible, with one exception</strong>.
<img src="../../../../images/progress-files/happy_face3.png" style="width:50px; height: 50px" class="transparent" /></p>
<p>The challenge with useful proof of work is that a proof of work
algorithm requires many properties:</p>
<ul>
<li>Hard to compute</li>
<li>Easy to verify</li>
<li>Does not depend on large amounts of external data</li>
<li>Can be efficiently computed in small "bite-sized" chunks</li>
</ul>
<p>Unfortunately, there are not many computations that are useful that
preserve all of these properties, and most computations that <em>do</em>
have all of those properties and are "useful" are only "useful" for far
too short a time to build a cryptocurrency around them.</p>
<p>However, there is one possible exception: zero-knowledge-proof
generation. Zero knowledge proofs of aspects of blockchain validity (eg.
<a
href="https://ethresear.ch/t/stark-proving-low-degree-ness-of-a-data-availability-root-some-analysis/6214">data
availability roots</a> for a simple example) are difficult to compute,
and easy to verify. Furthermore, they are durably difficult to compute;
if proofs of "highly structured" computation become too easy, one can
simply switch to verifying a blockchain's entire state transition, which
becomes extremely expensive due to the need to model the virtual machine
and random memory accesses.</p>
<p>Zero-knowledge proofs of blockchain validity provide great value to
users of the blockchain, as they can substitute the need to verify the
chain directly; <a href="https://codaprotocol.com/">Coda</a> is doing
this already, albeit with a simplified blockchain design that is heavily
optimized for provability. Such proofs can significantly assist in
improving the blockchain's safety and scalability. That said, the total
amount of computation that realistically needs to be done is still much
less than the amount that's currently done by proof of work miners, so
this would at best be an add-on for proof of stake blockchains, not a
full-on consensus algorithm.</p>
<blockquote style="background-color:#ffe4ff; padding-top: 18px; padding-bottom: 18px">
<h3>
<ol start="8" type="1">
<li>Proof of Stake
</h3>
Another approach to solving the mining centralization problem is to
abolish mining entirely, and move to some other mechanism for counting
the weight of each node in the consensus. The most popular alternative
under discussion to date is "proof of stake" - that is to say, instead
of treating the consensus model as "one unit of CPU power, one vote" it
becomes "one currency unit, one vote".
</blockquote></li>
</ol>
<p>Status: <strong>Great theoretical progress, pending more real-world
evaluation</strong>.
<img src="../../../../images/progress-files/happy_face1.png" style="width:50px; height: 50px" class="transparent" /></p>
<p>Near the end of 2014, it became clear to the proof of stake community
that some form of "weak subjectivity" <a
href="https://blog.ethereum.org/2014/11/25/proof-stake-learned-love-weak-subjectivity">is
unavoidable</a>. To maintain economic security, nodes need to obtain a
recent checkpoint extra-protocol when they sync for the first time, and
again if they go offline for more than a few months. This was a
difficult pill to swallow; many PoW advocates still cling to PoW
precisely because in a PoW chain the "head" of the chain can be
discovered with the only data coming from a trusted source being the
blockchain client software itself. PoS advocates, however, were willing
to swallow the pill, seeing the added trust requirements as not being
large. From there the path to proof of stake through long-duration
security deposits became clear.</p>
<p>Most interesting consensus algorithms today are fundamentally similar
to <a href="http://pmg.csail.mit.edu/papers/osdi99.pdf">PBFT</a>, but
replace the fixed set of validators with a dynamic list that anyone can
join by sending tokens into a system-level smart contract with
time-locked withdrawals (eg. a withdrawal might in some cases take up to
4 months to complete). In many cases (including ethereum 2.0), these
algorithms achieve "economic finality" by penalizing validators that are
caught performing actions that violate the protocol in certain ways (see
<a
href="https://medium.com/@VitalikButerin/a-proof-of-stake-design-philosophy-506585978d51">here</a>
for a philosophical view on what proof of stake accomplishes).</p>
<p>As of today, we have (among many other algorithms):</p>
<ul>
<li><strong>Casper FFG</strong>: <a
href="https://arxiv.org/abs/1710.09437">https://arxiv.org/abs/1710.09437</a></li>
<li><strong>Tendermint</strong>: <a
href="https://tendermint.com/docs/spec/consensus/consensus.html">https://tendermint.com/docs/spec/consensus/consensus.html</a></li>
<li><strong>HotStuff</strong>: <a
href="https://arxiv.org/abs/1803.05069">https://arxiv.org/abs/1803.05069</a></li>
<li><strong>Casper CBC</strong>: <a
href="../../../2018/12/05/cbc_casper.html">../../../2018/12/05/cbc_casper.html</a></li>
</ul>
<p>There continues to be ongoing refinement (eg. <a
href="https://ethresear.ch/t/analysis-of-bouncing-attack-on-ffg/6113">here</a>
and <a
href="https://ethresear.ch/t/saving-strategy-and-fmd-ghost/6226">here</a>)
. Eth2 phase 0, the chain that will implement FFG, is currently under
implementation and enormous progress has been made. Additionally,
Tendermint has been running, in the form of the <a
href="https://cosmos.bigdipper.live/validators">Cosmos chain</a> for
several months. Remaining arguments about proof of stake, in my view,
have to do with optimizing the economic incentives, and further
formalizing the <a
href="https://ethresear.ch/t/responding-to-51-attacks-in-casper-ffg/6363">strategy
for responding to 51% attacks</a>. Additionally, the <a
href="https://github.com/ethereum/eth2.0-specs/issues/701">Casper CBC
spec</a> could still use concrete efficiency improvements.</p>
<blockquote style="background-color:#ffe4ff; padding-top: 18px; padding-bottom: 18px">
<h3>
<ol start="9" type="1">
<li>Proof of Storage
</h3>
A third approach to the problem is to use a scarce computational
resource other than computational power or currency. In this regard, the
two main alternatives that have been proposed are storage and bandwidth.
There is no way in principle to provide an after-the-fact cryptographic
proof that bandwidth was given or used, so proof of bandwidth should
most accurately be considered a subset of social proof, discussed in
later problems, but proof of storage is something that certainly can be
done computationally. An advantage of proof-of-storage is that it is
completely ASIC-resistant; the kind of storage that we have in hard
drives is already close to optimal.
</blockquote></li>
</ol>
<p>Status: <strong>A lot of theoretical progress, though still a lot to
go, as well as more real-world evaluation</strong>.
<img src="../../../../images/progress-files/happy_face2.png" style="width:50px; height: 50px" class="transparent" /></p>
<p>There are a number of <a
href="https://en.wikipedia.org/wiki/Proof_of_space">blockchains planning
to use proof of storage</a> protocols, including <a
href="https://eprint.iacr.org/2017/893.pdf">Chia</a> and <a
href="https://filecoin.io/filecoin.pdf">Filecoin</a>. That said, these
algorithms have not been tested in the wild. My own main concern is
centralization: will these algorithms actually be dominated by smaller
users using spare storage capacity, or will they be dominated by large
mining farms?</p>
<h2 id="economics">Economics</h2>
<p><a name="numberten"></a></p>
<blockquote style="background-color:#ffe4ff; padding-top: 18px; padding-bottom: 18px">
<h3>
<ol start="10" type="1">
<li>Stable-value cryptoassets
</h3>
One of the main problems with Bitcoin is the issue of price volatility ...
Problem: construct a cryptographic asset with a stable price.
</blockquote></li>
</ol>
<p>Status: <strong>Some progress</strong>. &lt;img
src="../../../../images/progress-files/happy_face2.png"
style="width:50px; height: 50px" ]class="transparent" /&gt;</p>
<p><a href="https://makerdao.com/en/">MakerDAO</a> is now live, and has
been holding stable for nearly two years. It has survived a 93% drop in
the value of its underlying collateral asset (ETH), and there is now
more than $100 million in DAI issued. It has become a mainstay of the
Ethereum ecosystem, and many Ethereum projects have or are integrating
with it. Other synthetic token projects, such as <a
href="https://umaproject.org/">UMA</a>, are rapidly gaining steam as
well.</p>
<p>However, while the MakerDAO system has survived tough economic
conditions in 2019, the conditions were by no means the toughest that
could happen. In the past, Bitcoin has <a
href="https://fortune.com/2017/09/18/bitcoin-crash-history/">fallen by
75%</a> over the course of two days; the same may happen to ether or any
other collateral asset some day. Attacks on the underlying blockchain
are an even larger untested risk, especially if compounded by price
decreases at the same time. Another major challenge, and arguably the
larger one, is that the stability of MakerDAO-like systems is dependent
on some underlying oracle scheme. Different attempts at oracle systems
do exist (see #16), but the jury is still out on how well they can hold
up under large amounts of economic stress. So far, the collateral
controlled by MakerDAO has been lower than the value of the MKR token;
if this relationship reverses MKR holders may have a collective
incentive to try to "loot" the MakerDAO system. There are ways to try to
protect against such attacks, but they have not been tested in real
life.</p>
<blockquote style="background-color:#ffe4ff; padding-top: 18px; padding-bottom: 18px">
<h3>
<ol start="11" type="1">
<li>Decentralized Public Goods Incentivization
</h3>
One of the challenges in economic systems in general is the problem of
"public goods". For example, suppose that there is a scientific research
project which will cost $1 million to complete, and it is known that if
it is completed the resulting research will save one million people $5
each. In total, the social benefit is clear ... [but] from the point of
view of each individual person contributing does not make sense ... So
far, most problems to public goods have involved centralization
Additional Assumptions And Requirements: A fully trustworthy oracle
exists for determining whether or not a certain public good task has
been completed (in reality this is false, but this is the domain of
another problem)
</blockquote></li>
</ol>
<p>Status: <strong>Some progress</strong>.
<img src="../../../../images/progress-files/happy_face2.png" style="width:50px; height: 50px" class="transparent" /></p>
<p>The problem of funding public goods is generally understood to be
split into two problems: the funding problem (where to get funding for
public goods from) and the preference aggregation problem (how to
determine what is a genuine public good, rather than some single
individual's pet project, in the first place). This problem focuses
specifically on the former, assuming the latter is solved (see the <a
href="#numberfourteensic">"decentralized contribution metrics" section
below</a> for work on that problem).</p>
<p>In general, there haven't been large new breakthroughs here. There's
two major categories of solutions. First, we can try to elicit
individual contributions, giving people social rewards for doing so. My
own proposal for <a
href="../../../2017/03/11/a_note_on_charity.html">charity through
marginal price discrimination</a> is one example of this; another is the
anti-malaria donation badges on <a
href="https://peepeth.com/welcome">Peepeth</a>. Second, we can collect
funds from applications that have network effects. Within blockchain
land there are several options for doing this:</p>
<ul>
<li>Issuing coins</li>
<li>Taking a portion of transaction fees at protocol level (eg. through
<a href="https://github.com/ethereum/EIPs/issues/1559">EIP
1559</a>)</li>
<li>Taking a portion of transaction fees from some layer-2 application
(eg. Uniswap, or some scaling solution, or even state rent in an
execution environment in ethereum 2.0)</li>
<li>Taking a portion of other kinds of fees (eg. ENS registration)</li>
</ul>
<p>Outside of blockchain land, this is just the age-old question of how
to collect taxes if you're a government, and charge fees if you're a
business or other organization.</p>
<p><a name="numbertwelve"></a></p>
<blockquote style="background-color:#ffe4ff; padding-top: 18px; padding-bottom: 18px">
<h3>
<ol start="12" type="1">
<li>Reputation systems
</h3>
<strong>Problem:</strong> design a formalized reputation system,
including a score rep(A,B) -&gt; V where V is the reputation of B from
the point of view of A, a mechanism for determining the probability that
one party can be trusted by another, and a mechanism for updating the
reputation given a record of a particular open or finalized interaction.
</blockquote></li>
</ol>
<p>Status: <strong>Slow progress</strong>.
<img src="../../../../images/progress-files/happy_face3.png" style="width:50px; height: 50px" class="transparent" /></p>
<p>There hasn't really been much work on reputation systems since 2014.
Perhaps the best is the use of token curated registries to create
curated lists of trustable entities/objects; the <a
href="https://blog.kleros.io/erc20-becomes-part-of-the-token/">Kleros
ERC20 TCR</a> (yes, that's a <a
href="https://medium.com/@tokencuratedregistry/a-simple-overview-of-token-curated-registries-84e2b7b19a06">token-curated
registry</a> of legitimate ERC20 tokens) is one example, and there is
even an alternative interface to Uniswap (<a
href="http://uniswap.ninja">http://uniswap.ninja</a>) that uses it as
the backend to get the list of tokens and ticker symbols and logos from.
Reputation systems of the subjective variety have not really been tried,
perhaps because there is just not enough information about the "social
graph" of people's connections to each other that has already been
published to chain in some form. If such information starts to exist for
other reasons, then subjective reputation systems may become more
popular.</p>
<blockquote style="background-color:#ffe4ff; padding-top: 18px; padding-bottom: 18px">
<h3>
<ol start="13" type="1">
<li>Proof of excellence
</h3>
One interesting, and largely unexplored, solution to the problem of
[token] distribution specifically (there are reasons why it cannot be so
easily used for mining) is using tasks that are socially useful but
require original human-driven creative effort and talent. For example,
one can come up with a "proof of proof" currency that rewards players
for coming up with mathematical proofs of certain theorems
</blockquote></li>
</ol>
<p>Status: <strong>No progress, problem is largely forgotten</strong>.
<img src="../../../../images/progress-files/happy_face5.png" style="width:50px; height: 50px" class="transparent" /></p>
<p>The main alternative approach to token distribution that has instead
become popular is <a
href="https://en.wikipedia.org/wiki/Airdrop_%28cryptocurrency%29">airdrops</a>;
typically, tokens are distributed at launch either proportionately to
existing holdings of some other token, or based on some other metric
(eg. as in the <a
href="https://help.namebase.io/article/4vchu01mec-handshake-airdrop-101">Handshake
airdrop</a>). Verifying human creativity directly has not really been
attempted, and with recent progress on AI the problem of creating a task
that only humans can do but computers can verify may well be too
difficult.</p>
<p><a name="numberfifteensic"></a></p>
<blockquote style="background-color:#ffe4ff; padding-top: 18px; padding-bottom: 18px">
<h3>
15 [sic]. Anti-Sybil systems
</h3>
A problem that is somewhat related to the issue of a reputation system
is the challenge of creating a "unique identity system" - a system for
generating tokens that prove that an identity is not part of a Sybil
attack ... However, we would like to have a system that has nicer and more
egalitarian features than "one-dollar-one-vote"; arguably,
one-person-one-vote would be ideal.
</blockquote>
<p>Status: <strong>Some progress</strong>.
<img src="../../../../images/progress-files/happy_face2.png" style="width:50px; height: 50px" class="transparent" /></p>
<p>There have been quite a few attempts at solving the unique-human
problem. Attempts that come to mind include (incomplete list!):</p>
<ul>
<li><strong>HumanityDAO</strong>: <a
href="https://www.humanitydao.org/">https://www.humanitydao.org/</a></li>
<li><strong>Pseudonym parties</strong>: <a
href="https://bford.info/pub/net/sybil.pdf">https://bford.info/pub/net/sybil.pdf</a></li>
<li><strong>POAP</strong> ("proof of attendance protocol"): <a
href="https://www.poap.xyz/">https://www.poap.xyz/</a></li>
<li><strong>BrightID</strong>: <a
href="https://www.brightid.org/">https://www.brightid.org/</a></li>
</ul>
<p>With the growing interest in techniques like <a
href="https://en.wikipedia.org/wiki/Quadratic_voting">quadratic
voting</a> and <a
href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3243656">quadratic
funding</a>, the need for some kind of human-based anti-sybil system
continues to grow. Hopefully, ongoing development of these techniques
and new ones can come to meet it.</p>
<a name="numberfourteensic"></a>
<blockquote style="background-color:#ffe4ff; padding-top: 18px; padding-bottom: 18px">
<h3>
14 [sic]. Decentralized contribution metrics
</h3>
Incentivizing the production of public goods is, unfortunately, not the
only problem that centralization solves. The other problem is
determining, first, which public goods are worth producing in the first
place and, second, determining to what extent a particular effort
actually accomplished the production of the public good. This challenge
deals with the latter issue.
</blockquote>
<p>Status: <strong>Some progress, some change in focus</strong>.
<img src="../../../../images/progress-files/happy_face6.png" style="width:50px; height: 50px" class="transparent" /></p>
<p>More recent work on determining value of public-good contributions
does not try to separate determining tasks and determining quality of
completion; the reason is that in practice the two are difficult to
separate. Work done by specific teams tends to be non-fungible and
subjective enough that the most reasonable approach is to look at
relevance of task and quality of performance as a single package, and
use the same technique to evaluate both.</p>
<p>Fortunately, there has been great progress on this, particularly with
the discovery of <a
href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3243656">quadratic
funding</a>. Quadratic funding is a mechanism where individuals can make
donations to projects, and then based on the number of people who
donated and how much they donated, a formula is used to calculate how
much they would have donated if they were perfectly coordinated with
each other (ie. took each other's interests into account and did not
fall prey to the tragedy of the commons). The difference between amount
would-have-donated and amount actually donated for any given project is
given to that project as a subsidy from some central pool (see #11 for
where the central pool funding could come from). Note that this
mechanism focuses on satisfying the values of some community, not on
satisfying some given goal regardless of whether or not anyone cares
about it. Because of the <a
href="https://wiki.lesswrong.com/wiki/Complexity_of_value">complexity of
values</a> problem, this approach is likely to be much more robust to
unknown unknowns.</p>
<p>Quadratic funding has even been tried in real life with considerable
success in the <a href="../../../2019/10/24/gitcoin.html">recent gitcoin
quadratic funding round</a>. There has also been some incremental
progress on improving quadratic funding and similar mechanisms;
particularly, <a
href="https://ethresear.ch/t/pairwise-coordination-subsidies-a-new-quadratic-funding-design/5553">pairwise-bounded
quadratic funding</a> to mitigate collusion. There has also been work on
specification and implementation of <a
href="https://ethresear.ch/t/minimal-anti-collusion-infrastructure/5413">bribe-resistant</a>
voting technology, preventing users from proving to third parties who
they voted for; this prevents many kinds of collusion and bribe
attacks.</p>
<p><a name="numbersixteen"></a></p>
<blockquote style="background-color:#ffe4ff; padding-top: 18px; padding-bottom: 18px">
<h3>
<ol start="16" type="1">
<li>Decentralized success metrics
</h3>
Problem: come up with and implement a decentralized method for measuring
numerical real-world variables ... the system should be able to measure
anything that humans can currently reach a rough consensus on (eg. price
of an asset, temperature, global CO2 concentration)
</blockquote></li>
</ol>
<p>Status: <strong>Some progress</strong>.
<img src="../../../../images/progress-files/happy_face2.png" style="width:50px; height: 50px" class="transparent" /></p>
<p>This is now generally just called "the oracle problem". The largest
known instance of a decentralized oracle running is <a
href="https://www.augur.net/">Augur</a>, which has processed outcomes
for millions of dollars of bets. <a
href="https://medium.com/@tokencuratedregistry/a-simple-overview-of-token-curated-registries-84e2b7b19a06">Token
curated registries</a> such as the <a
href="https://tokens.kleros.io/tokens">Kleros TCR for tokens</a> are
another example. However, these systems still have not seen a real-world
test of the forking mechanism (search for "subjectivocracy" <a
href="https://blog.ethereum.org/2015/02/14/subjectivity-exploitability-tradeoff/">here</a>)
either due to a highly controversial question or due to an attempted 51%
attack. There is also research on the oracle problem happening outside
of the blockchain space in the form of the "<a
href="https://www2.cs.duke.edu/courses/spring17/compsci590.2/peer_prediction.pdf">peer
prediction</a>" literature; see <a
href="https://arxiv.org/abs/1911.00272">here</a> for a very recent
advancement in the space.</p>
<p>Another looming challenge is that people want to rely on these
systems to guide transfers of quantities of assets larger than the
economic value of the system's native token. In these conditions, token
holders in theory have the incentive to collude to give wrong answers to
steal the funds. In such a case, the system would fork and the original
system token would likely become valueless, but the original system
token holders would still get away with the returns from whatever asset
transfer they misdirected. Stablecoins (see <a
href="#numberten">#10</a>) are a particularly egregious case of this.
One approach to solving this would be a system that assumes that
altruistically honest data providers do exist, and creating a mechanism
to identify them, and only allowing them to churn slowly so that if
malicious ones start getting voted in the users of systems that rely on
the oracle can first complete an orderly exit. In any case, more
development of oracle tech is very much an important problem.</p>
<h3 id="new-problems">New problems</h3>
<p>If I were to write the hard problems list again in 2019, some would
be a continuation of the above problems, but there would be significant
changes in emphasis, as well as significant new problems. Here are a few
picks:</p>
<ul>
<li><strong>Cryptographic obfuscation</strong>: same as <a
href="#numberfour">#4</a> above</li>
<li><strong>Ongoing work on post-quantum cryptography</strong>: both
hash-based as well as based on post-quantum-secure "structured"
mathematical objects, eg. elliptic curve isogenies, lattices...</li>
<li><strong>Anti-collusion infrastructure</strong>: ongoing work and
refinement of <a
href="https://ethresear.ch/t/minimal-anti-collusion-infrastructure/5413">https://ethresear.ch/t/minimal-anti-collusion-infrastructure/5413</a>,
including adding privacy against the operator, adding multi-party
computation in a maximally practical way, etc.</li>
<li><strong>Oracles</strong>: same as <a href="#numbersixteen">#16</a>
above, but removing the emphasis on "success metrics" and focusing on
the general "get real-world data" problem</li>
<li><strong>Unique-human identities</strong> (or, more realistically,
semi-unique-human identities): same as what was written as <a
href="#numberfifteensic">#15</a> above, but with an emphasis on a less
"absolute" solution: it should be much harder to get two identities than
one, but making it impossible to get multiple identities is both
impossible and potentially harmful even if we do succeed</li>
<li><strong>Homomorphic encryption and multi-party computation</strong>:
ongoing improvements are still required for practicality</li>
<li><strong>Decentralized governance mechanisms</strong>: DAOs are cool,
but current DAOs are still very primitive; we can do better</li>
<li><strong>Fully formalizing responses to PoS 51% attacks</strong>:
ongoing work and refinement of <a
href="https://ethresear.ch/t/responding-to-51-attacks-in-casper-ffg/6363">https://ethresear.ch/t/responding-to-51-attacks-in-casper-ffg/6363</a></li>
<li><strong>More sources of public goods funding</strong>: the ideal is
to charge for congestible resources inside of systems that have network
effects (eg. transaction fees), but doing so in decentralized systems
requires public legitimacy; hence this is a social problem along with
the technical one of finding possible sources</li>
<li><strong>Reputation systems</strong>: same as <a
href="#numbertwelve">#12</a> above</li>
</ul>
<p>In general, base-layer problems are slowly but surely decreasing, but
application-layer problems are only just getting started.</p>
 </div> 