

<!DOCTYPE html>
<html>
<meta charset="UTF-8">
<style>
@media (prefers-color-scheme: dark) {
    body {
        background-color: #1c1c1c;
        color: white;
    }
    .markdown-body table tr {
        background-color: #1c1c1c;
    }
    .markdown-body table tr:nth-child(2n) {
        background-color: black;
    }
}
</style>



<link rel="alternate" type="application/rss+xml" href="../../../../feed.xml" title="The promise and challenges of crypto + AI applications">



<link rel="stylesheet" type="text/css" href="../../../../css/common-vendor.b8ecfc406ac0b5f77a26.css">
<link rel="stylesheet" type="text/css" href="../../../../css/fretboard.f32f2a8d5293869f0195.css">
<link rel="stylesheet" type="text/css" href="../../../../css/pretty.0ae3265014f89d9850bf.css">
<link rel="stylesheet" type="text/css" href="../../../../css/pretty-vendor.83ac49e057c3eac4fce3.css">
<link rel="stylesheet" type="text/css" href="../../../../css/global.css">
<link rel="stylesheet" type="text/css" href="../../../../css/misc.css">

<script type="text/x-mathjax-config">
<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\(', '\)']]
  },
  svg: {
    fontCache: 'global',
  }
};
</script>
<script type="text/javascript" id="MathJax-script" async
  src="../../../../scripts/tex-svg.js">
</script>

<style>
</style>

<div id="doc" class="container-fluid markdown-body comment-enabled" data-hard-breaks="true">

<div id="color-mode-switch">
  <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2">
    <path stroke-linecap="round" stroke-linejoin="round" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364l-.707-.707M6.343 6.343l-.707-.707m12.728 0l-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z" />
  </svg>
  <input type="checkbox" id="switch" />
  <label for="switch">Dark Mode Toggle</label>
  <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2">
    <path stroke-linecap="round" stroke-linejoin="round" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003 0 0012 21a9.003 9.003 0 008.354-5.646z" />
  </svg>
</div>

<script type="text/javascript">
  // Update root html class to set CSS colors
  const toggleDarkMode = () => {
    const root = document.querySelector('html');
    root.classList.toggle('dark');
  }

  // Update local storage value for colorScheme
  const toggleColorScheme = () => {
    const colorScheme = localStorage.getItem('colorScheme');
    if (colorScheme === 'light') localStorage.setItem('colorScheme', 'dark');
    else localStorage.setItem('colorScheme', 'light');
  }

  // Set toggle input handler
  const toggle = document.querySelector('#color-mode-switch input[type="checkbox"]');
  if (toggle) toggle.onclick = () => {
    toggleDarkMode();
    toggleColorScheme();
  }

  // Check for color scheme on init
  const checkColorScheme = () => {
    const colorScheme = localStorage.getItem('colorScheme');
    // Default to light for first view
    if (colorScheme === null || colorScheme === undefined) localStorage.setItem('colorScheme', 'light');
    // If previously saved to dark, toggle switch and update colors
    if (colorScheme === 'dark') {
      toggle.checked = true;
      toggleDarkMode();
    }
  }
  checkColorScheme();
</script>

<meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="The promise and challenges of crypto + AI applications" />
<meta name="twitter:image" content="http://vitalik.ca/images/icon.png" />


<br>
<h1 style="margin-bottom:7px"> The promise and challenges of crypto + AI applications </h1>
<small style="float:left; color: #888"> 2024 Jan 30 </small>
<small style="float:right; color: #888"><a href="../../../../index.html">See all posts</a></small>
<br> <br> <br>
<title> The promise and challenges of crypto + AI applications </title>

<p><em>Special thanks to the Worldcoin and Modulus Labs teams, Xinyuan
Sun, Martin Koeppelmann and Illia Polosukhin for feedback and
discussion.</em></p>
<p>Many people over the years have asked me a similar question: what are
the <em>intersections</em> between crypto and AI that I consider to be
the most fruitful? It's a reasonable question: crypto and AI are the two
main deep (software) technology trends of the past decade, and it just
feels like there <em>must</em> be some kind of connection between the
two. It's easy to come up with synergies at a superficial vibe level:
crypto decentralization can <a
href="https://im0xalpha.notion.site/AI-to-the-Left-Crypto-to-the-Right-914c101cfc314659a18dbbabc89d152b">balance
out AI centralization</a>, AI is opaque and crypto brings transparency,
AI needs data and blockchains are good for storing and tracking data.
But over the years, when people would ask me to dig a level deeper and
talk about specific applications, my response has been a disappointing
one: "yeah there's a few things but not that much".</p>
<p>In the last three years, with the rise of much more powerful AI in
the form of modern <a
href="https://en.wikipedia.org/wiki/Large_language_model">LLMs</a>, and
the rise of much more powerful crypto in the form of not just blockchain
scaling solutions but also <a
href="../../../2021/01/26/snarks.html">ZKPs</a>, <a
href="../../../2020/07/20/homomorphic.html">FHE</a>, (two-party and
N-party) <a
href="https://en.wikipedia.org/wiki/Secure_multi-party_computation">MPC</a>,
I am starting to see this change. There are indeed some promising
applications of AI inside of blockchain ecosystems, or <a
href="https://www.lesswrong.com/posts/DvmDgpKmd3ftPDtzr/cryptographic-and-auxiliary-approaches-relevant-for-ai">AI
together with cryptography</a>, though it is important to be careful
about how the AI is applied. A particular challenge is: in cryptography,
open source is the only way to make something truly secure, but in AI, a
model (or even its training data) being open <em>greatly increases</em>
its vulnerability to <a
href="https://en.wikipedia.org/wiki/Adversarial_machine_learning">adversarial
machine learning</a> attacks. This post will go through a classification
of different ways that crypto + AI could intersect, and the prospects
and challenges of each category.</p>
<center>
<p><br></p>
<p><img src="../../../../images/cryptoai/ueth.png" /></p>
<p><em>A high-level summary of crypto+AI intersections from a <a
href="https://blog.ueth.org/p/when-giants-collide-exploring-the">uETH
blog post</a>. But what does it take to actually realize any of these
synergies in a concrete application?</em></p>
</center>
<p><br></p>
<h2 id="the-four-major-categories">The four major categories</h2>
<p>AI is a very broad concept: you can think of "AI" as being the set of
algorithms that you create not by specifying them explicitly, but rather
by stirring a big computational soup and putting in some kind of
optimization pressure that nudges the soup toward producing algorithms
with the properties that you want. This description should definitely
not be taken dismissively: it <a
href="https://en.wikipedia.org/wiki/Primordial_soup">includes</a> the <a
href="https://en.wikipedia.org/wiki/Natural_selection">process</a> that
<a
href="https://docs.google.com/document/d/1IJ6Sr-gPeXdSJugFulwIpvavc0atjHGM82QjIfUSBGQ/edit">created</a>
us humans in the first place! But it does mean that AI algorithms have
some common properties: their ability to do things that are extremely
powerful, together with limits in our ability to know or understand
what's going on under the hood.</p>
<p>There are many ways to categorize AI; for the purposes of this post,
which talks about interactions between AI and blockchains (which have
been described as a platform for <a
href="https://medium.com/@virgilgr/ethereum-is-game-changing-technology-literally-d67e01a01cf8">creating
"games"</a>), I will categorize it as follows:</p>
<ul>
<li><strong>AI as a player in a game [highest viability]:</strong> AIs
participating in mechanisms where the ultimate source of the incentives
comes from a protocol with human inputs.</li>
<li><strong>AI as an interface to the game [high potential, but with
risks]:</strong> AIs helping users to understand the crypto world around
them, and to ensure that their behavior (ie. signed messages and
transactions) matches their intentions and they do not get tricked or
scammed.</li>
<li><strong>AI as the rules of the game [tread very carefully]:</strong>
blockchains, DAOs and similar mechanisms directly calling into AIs.
Think eg. "AI judges"</li>
<li><strong>AI as the objective of the game [longer-term but
intriguing]:</strong> designing blockchains, DAOs and similar mechanisms
with the goal of constructing and maintaining an AI that could be used
for other purposes, using the crypto bits either to better incentivize
training or to prevent the AI from leaking private data or being
misused.</li>
</ul>
<p>Let us go through these one by one.</p>
<h2 id="ai-as-a-player-in-a-game">AI as a player in a game</h2>
<p>This is actually a category that has existed for nearly a decade, at
least since <a
href="https://coinmarketcap.com/academy/article/the-evolution-of-decentralized-exchanges">on-chain
decentralized exchanges (DEXes)</a> started to see significant use. Any
time there is an exchange, there is an opportunity to make money through
arbitrage, and bots can do arbitrage much better than humans can. This
use case has existed for a long time, even with much simpler AIs than
what we have today, but ultimately it is a very real AI + crypto
intersection. More recently, we have seen MEV arbitrage bots <a
href="https://www.paradigm.xyz/2020/08/ethereum-is-a-dark-forest">often
exploiting each other</a>. Any time you have a blockchain application
that involves auctions or trading, you are going to have arbitrage
bots.</p>
<p>But AI arbitrage bots are only the first example of a much bigger
category, which I expect will soon start to include many other
applications. Meet AIOmen, a <a href="https://aiomen.eth.limo/">demo of
a prediction market where AIs are players</a>:</p>
<center>
<p><br></p>
<p><img src="../../../../images/cryptoai/aiomen.png" /></p>
</center>
<p><br></p>
<p>Prediction markets have been a holy grail of epistemics technology
for a long time; I was excited about using prediction markets as an
input for governance ("futarchy") <a
href="https://blog.ethereum.org/2014/08/21/introduction-futarchy">back
in 2014</a>, and <a href="../../../2021/02/18/election.html">played
around with them extensively</a> in the last election as well as <a
href="https://warpcast.com/vitalik.eth/0xec367991">more recently</a>.
But so far prediction markets have not taken off <em>too much</em> in
practice, and there is a series of commonly given reasons why: the
largest participants are often irrational, people with the right
knowledge are not willing to take the time and bet unless <em>a lot</em>
of money is involved, markets are often thin, etc.</p>
<p>One response to this is to point to ongoing UX improvements in <a
href="https://polymarket.com/">Polymarket</a> or other new prediction
markets, and hope that they will succeed where previous iterations have
failed. After all, the story goes, people are willing to bet <a
href="https://www.globenewswire.com/en/news-release/2023/10/19/2763467/0/en/Sports-Betting-Market-is-set-to-reach-USD-231-2-billion-by-2032-with-a-notable-11-1-growth-rate-CAGR.html">tens
of billions on sports</a>, so why wouldn't people throw in enough money
betting on US elections or <a
href="https://en.wikipedia.org/wiki/LK-99">LK99</a> that it starts to
make sense for the serious players to start coming in? But this argument
must contend with the fact that, well, previous iterations <em>have</em>
failed to get to this level of scale (at least compared to their
proponents' dreams), and so it seems like you need <em>something
new</em> to make prediction markets succeed. And so a different response
is to point to one specific feature of prediction market ecosystems that
we can expect to see in the 2020s that we did not see in the 2010s:
<strong>the possibility of ubiquitous participation by AIs</strong>.</p>
<p>AIs are willing to work for less than $1 per hour, and have the
knowledge of an encyclopedia - and if that's not enough, they can even
be integrated with real-time web search capability. If you make a
market, and put up a liquidity subsidy of $50, humans will not care
enough to bid, but thousands of AIs will easily swarm all over the
question and make the best guess they can. The incentive to do a good
job on any one question may be tiny, but the incentive to make an AI
that makes good predictions <em>in general</em> may be in the millions.
Note that potentially, <strong>you don't even need the humans to
adjudicate most questions</strong>: you can use a multi-round dispute
system similar to <a
href="https://augur.gitbook.io/help-center/disputing-explained">Augur</a>
or Kleros, where AIs would also be the ones participating in earlier
rounds. Humans would only need to respond in those few cases where a
series of escalations have taken place and large amounts of money have
been committed by both sides.</p>
<p>This is a powerful primitive, because once a "prediction market" can
be made to work on such a microscopic scale, you can reuse the
"prediction market" primitive for many other kinds of questions:</p>
<ul>
<li><strong>Is this social media post acceptable under [terms of
use]?</strong></li>
<li><strong>What will happen to the price of stock X (eg. see <a
href="https://numer.ai/">Numerai</a>)</strong></li>
<li><strong>Is this account that is currently messaging me actually Elon
Musk?</strong></li>
<li><strong>Is this work submission on an online task marketplace
acceptable?</strong></li>
<li><strong>Is the dapp at https://examplefinance.network a
scam?</strong></li>
<li><strong>Is <code>0x1b54....98c3</code> actually the address of the
"Casinu Inu" ERC20 token?</strong></li>
</ul>
<p>You may notice that a lot of these ideas go in the direction of what
I called "<a href="../../../2023/11/27/techno_optimism.html#info">info
defense</a>" in my writings on "d/acc". Broadly defined, the question
is: how do we help users tell apart true and false information and
detect scams, without empowering a centralized authority to decide right
and wrong who might then abuse that position? At a micro level, the
answer can be "AI". But at a macro level, the question is: who builds
the AI? AI is a reflection of the process that created it, and so cannot
avoid having biases. <strong>Hence, there is a need for a higher-level
game which adjudicates how well the different AIs are doing, where AIs
can participate as players in the game</strong>.</p>
<p>This usage of AI, where AIs participate in a mechanism where they get
ultimately rewarded or penalized (probabilistically) by an on-chain
mechanism that gathers inputs from humans (call it decentralized
market-based <a
href="https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback">RLHF</a>?),
is something that I think is really worth looking into. Now is the right
time to look into use cases like this more, because blockchain scaling
is finally succeeding, making "micro-" anything finally viable on-chain
when it was often not before.</p>
<p>A related category of applications goes in the direction of highly
autonomous agents <a href="https://arxiv.org/pdf/2311.07815.pdf">using
blockchains to better cooperate</a>, whether through payments or through
using smart contracts to make credible commitments.</p>
<h2 id="ai-as-an-interface-to-the-game">AI as an interface to the
game</h2>
<p>One idea that I brought up in my <a
href="../../../2023/11/27/techno_optimism.html#dacc">writings on
<d/acc></a> is the idea that there is a market opportunity to write
user-facing software that would protect users' interests by interpreting
and identifying dangers in the online world that the user is navigating.
One already-existing example of this is Metamask's scam detection
feature:</p>
<center>
<p><br></p>
<p><img src="../../../../images/cryptoai/scamblock.png" /></p>
</center>
<p><br></p>
<p>Another example is the <a href="https://rabby.io/">Rabby wallet's</a>
simulation feature, which shows the user the expected consequences of
the transaction that they about to sign.</p>
<center>
<p><br></p>
<p><img src="../../../../images/cryptoai/rabby.png" /></p>
<p><em>Rabby explaining to me the consequences of signing a transaction
to trade all of my "BITCOIN" (the ticker of an ERC20 memecoin whose full
name is apparently "<a
href="https://hpos10i.com/">HarryPotterObamaSonic10Inu</a>") for
ETH.</em></p>
<p><em>Edit 2024.02.02: an earlier version of this post referred to this
token as a scam trying to impersonate bitcoin. It is not; it is a
memecoin. Apologies for the confusion.</em></p>
</center>
<p><br></p>
<p>Potentially, these kinds of tools could be super-charged with AI. AI
could give a much richer human-friendly explanation of what kind of dapp
you are participating in, the consequences of more complicated
operations that you are signing, whether or not a particular token is
genuine (eg. <code>BITCOIN</code> is not just a string of characters,
it's normally the name of a major cryptocurrency, which is not an ERC20
token and which has a price waaaay higher than $0.045, and a modern LLM
would know that), and so on. There are projects starting to go all the
way out in this direction (eg. the <a
href="https://python.langchain.com/docs/integrations/document_loaders/blockchain">LangChain
wallet</a>, which uses AI as a <em>primary</em> interface). My own
opinion is that pure AI interfaces are probably too risky at the moment
as it increases the risk of <a
href="https://hai.stanford.edu/news/hallucinating-law-legal-mistakes-large-language-models-are-pervasive"><em>other</em>
kinds of errors</a>, but AI complementing a more conventional interface
is getting very viable.</p>
<p>There is one particular risk worth mentioning. I will get into this
more in the section on "AI as rules of the game" below, but <strong>the
general issue is adversarial machine learning: if a user has access to
an AI assistant inside an open-source wallet, the bad guys will have
access to that AI assistant too, and so they will have unlimited
opportunity to optimize their scams to not trigger that wallet's
defenses</strong>. All modern AIs have bugs somewhere, and it's not too
hard for a training process, even one with only <a
href="https://posts.specterops.io/learning-machine-learning-part-3-attacking-black-box-models-3efffc256909">limited
access to the model</a>, to find them.</p>
<p>This is where "AIs participating in on-chain micro-markets" works
better: each individual AI is vulnerable to the same risks, but you're
intentionally creating an open ecosystem of dozens of people constantly
iterating and improving them on an ongoing basis. Furthermore, each
individual AI is closed: the security of the system comes from the
openness of the rules of the <em>game</em>, not the internal workings of
each <em>player</em>.</p>
<p>Summary: <strong>AI can help users understand what's going on in
plain language, it can serve as a real-time tutor, it can protect users
from mistakes, but be warned when trying to use it directly against
malicious misinformers and scammers.</strong></p>
<h2 id="ai-as-the-rules-of-the-game">AI as the rules of the game</h2>
<p>Now, we get to the application that a lot of people are excited
about, but that I think is the most risky, and where we need to tread
the most carefully: what I call AIs being part of the rules of the game.
This ties into excitement among mainstream political elites about "AI
judges" (eg. see <a
href="https://www.worldgovernmentsummit.org/observer/articles/2017/detail/could-an-ai-ever-replace-a-judge-in-court">this
article</a> on the website of the "World Government Summit"), and there
are analogs of these desires in blockchain applications. If a
blockchain-based smart contract or a DAO needs to make a subjective
decision (eg. is a particular work product acceptable in a work-for-hire
contract? Which is the right interpretation of a natural-language
constitution like the Optimism <a
href="https://optimism.mirror.xyz/JfVOJ1Ng2l5H6JbIAtfOcYBKa4i9DyRTUJUuOqDpjIs">Law
of Chains</a>?), could you make an AI simply be part of the contract or
DAO to help enforce these rules?</p>
<p>This is where <a
href="https://www.toptal.com/machine-learning/adversarial-machine-learning-tutorial">adversarial
machine learning</a> is going to be an extremely tough challenge. The
basic two-sentence argument why is as follows:</p>
<p><br></p>
<blockquote>
<p><strong>If an AI model that plays a key role in a mechanism is
closed, you can't verify its inner workings, and so it's no better than
a centralized application. If the AI model is open, then an attacker can
download and simulate it locally, and design heavily optimized attacks
to trick the model, which they can then replay on the live
network.</strong></p>
</blockquote>
<center>
<p><br></p>
<p><img src="../../../../images/cryptoai/adverselearning.png" /></p>
<p><br></p>
<p><em>Adversarial machine learning example. Source: <a
href="https://www.researchgate.net/figure/An-example-of-the-adversarial-effect-in-machine-learning_fig4_361645628">researchgate.net</a></em></p>
</center>
<p><br></p>
<p>Now, frequent readers of this blog (or denizens of the cryptoverse)
might be getting ahead of me already, and thinking: but wait! We have
fancy zero knowledge proofs and other really cool forms of cryptography.
Surely we can do some crypto-magic, and hide the inner workings of the
model so that attackers can't optimize attacks, but at the same time <a
href="../../../2021/01/26/snarks.html">prove</a> that the model is being
executed correctly, and was constructed using a reasonable training
process on a reasonable set of underlying data!</p>
<p>Normally, this is <em>exactly</em> the type of thinking that I
advocate both on this blog and in my other writings. But in the case of
AI-related computation, there are two major objections:</p>
<ol type="1">
<li><strong>Cryptographic overhead</strong>: it's much less efficient to
do something inside a SNARK (or MPC or...) than it is to do it "in the
clear". Given that AI is very computationally-intensive already, is
doing AI inside of cryptographic black boxes even computationally
viable?</li>
<li><strong>Black-box adversarial machine learning attacks</strong>:
there are ways to optimize attacks against AI models <a
href="https://posts.specterops.io/learning-machine-learning-part-3-attacking-black-box-models-3efffc256909">even
without knowing much</a> about the model's internal workings. And if you
hide <em>too much</em>, you risk making it too easy for whoever chooses
the training data to corrupt the model with <a
href="https://arxiv.org/abs/2310.13828">poisoning</a> <a
href="https://arxiv.org/pdf/2305.14851.pdf">attacks</a>.</li>
</ol>
<p>Both of these are complicated rabbit holes, so let us get into each
of them in turn.</p>
<h2 id="cryptographic-overhead">Cryptographic overhead</h2>
<p>Cryptographic gadgets, especially general-purpose ones like ZK-SNARKs
and MPC, have a high overhead. An Ethereum block takes a few hundred
milliseconds for a client to verify directly, but generating a ZK-SNARK
to prove the correctness of such a block can take hours. The typical
overhead of other cryptographic gadgets, like MPC, can be even worse. AI
computation is expensive already: the most powerful LLMs can output
individual words only a little bit faster than human beings can read
them, not to mention the often multimillion-dollar computational costs
of <em>training</em> the models. The difference in quality between
top-tier models and the models that try to economize much more on <a
href="https://epochai.org/blog/how-predictable-is-language-model-benchmark-performance">training
cost</a> or <a
href="https://www.researchgate.net/figure/Classification-accuracy-versus-number-of-parameters-The-blue-color-curves-are-based-on_fig2_339374321">parameter
count</a> is large. At first glance, this is a very good reason to be
suspicious of the whole project of trying to add guarantees to AI by
wrapping it in cryptography.</p>
<p>Fortunately, though, <strong>AI is a <em>very specific type</em> of
computation, which makes it amenable to all kinds of
optimizations</strong> that more "unstructured" types of computation
like ZK-EVMs cannot benefit from. Let us examine the basic structure of
an AI model:</p>
<center>
<p><br></p>
<p><img src="../../../../images/cryptoai/ainets.png" /></p>
</center>
<p><br></p>
<p>Usually, an AI model mostly consists of a series of matrix
multiplications interspersed with per-element non-linear operations such
as the <a
href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU</a>
function (<code>y = max(x, 0)</code>). Asymptotically, matrix
multiplications take up most of the work: multiplying two
<code>N*N</code> matrices <a
href="https://en.wikipedia.org/wiki/Strassen_algorithm">takes <span
class="math inline">\(O(N^{2.8})\)</span> time</a>, whereas the number
of non-linear operations is much smaller. <strong>This is really
convenient for cryptography, because many forms of cryptography can do
linear operations (which matrix multiplications are, at least if you
encrypt the model but not the inputs to it) almost "for
free"</strong>.</p>
<p>If you are a cryptographer, you've probably already heard of a
similar phenomenon in the context of <a
href="../../../2020/07/20/homomorphic.html">homomorphic encryption</a>:
performing <em>additions</em> on encrypted ciphertexts is really easy,
but <em>multiplications</em> are incredibly hard and we did not figure
out any way of doing it at all with unlimited depth until 2009.</p>
<p>For ZK-SNARKs, the equivalent is <a
href="https://eprint.iacr.org/2013/351.pdf">protocols like this one from
2013</a>, which show a <em>less than 4x</em> overhead on proving matrix
multiplications. Unfortunately, the overhead on the non-linear layers
still ends up being significant, and the best implementations in
practice show overhead of around 200x. But there is hope that this can
be greatly decreased through further research; see <a
href="https://www.youtube.com/watch?v=tPZDIzrsg-E">this presentation
from Ryan Cao</a> for a recent approach based on GKR, and my own <a
href="https://ethresear.ch/t/using-gkr-inside-a-snark-to-reduce-the-cost-of-hash-verification-down-to-3-constraints/7550/3">simplified
explanation of how the main component of GKR works</a>.</p>
<p>But for many applications, we don't just want to <em>prove</em> that
an AI output was computed correctly, we also want to <em>hide the
model</em>. There are naive approaches to this: you can split up the
model so that a different set of servers redundantly store each layer,
and hope that some of the servers leaking some of the layers doesn't
leak too much data. But there are also surprisingly effective forms of
<a href="https://eprint.iacr.org/2018/403.pdf">specialized multi-party
computation</a>.</p>
<center>
<p><br></p>
<p><img src="../../../../images/cryptoai/mpcnets.png" /></p>
<p><em>A simplified diagram of one of these approaches, keeping the
model private but making the inputs public. If we want to keep the model
<em>and</em> the inputs private, we can, though it gets a bit more
complicated: see pages 8-9 of <a
href="https://eprint.iacr.org/2018/403.pdf">the paper</a>.</em></p>
</center>
<p><br></p>
<p>In both cases, the moral of the story is the same: <strong>the
greatest part of an AI computation is matrix multiplications, for which
it is possible to make <em>very efficient</em> ZK-SNARKs or MPCs (or
even FHE), and so the total overhead of putting AI inside cryptographic
boxes is surprisingly low</strong>. Generally, it's the non-linear
layers that are the greatest bottleneck despite their smaller size;
perhaps newer techniques like <a
href="https://hackernoon.com/exploring-lookup-arguments">lookup
arguments</a> can help.</p>
<h2 id="black-box-adversarial-machine-learning">Black-box adversarial
machine learning</h2>
<p>Now, let us get to the other big problem: the kinds of attacks that
you can do <em>even if</em> the contents of the model are kept private
and you only have "API access" to the model. Quoting a <a
href="https://arxiv.org/pdf/1605.07277.pdf">paper from 2016</a>:</p>
<blockquote>
<p>Many machine learning models are vulnerable to adversarial examples:
inputs that are specially crafted to cause a machine learning model to
produce an incorrect output. <strong>Adversarial examples that affect
one model often affect another model, even if the two models have
different architectures or were trained on different training sets, so
long as both models were trained to perform the same task</strong>. An
attacker may therefore train their own substitute model, craft
adversarial examples against the substitute, and transfer them to a
victim model, with very little information about the victim.</p>
</blockquote>
<center>
<p><br></p>
<p><img src="../../../../images/cryptoai/inferredclassifier.png" /></p>
<p><em>Use black-box access to a "target classifier" to train and refine
your own locally stored "inferred classifier". Then, locally generate
optimized attacks against the inferred classifier. It turns out these
attacks will often also work against the original target classifier. <a
href="https://arxiv.org/pdf/1811.01811.pdf">Diagram source</a>.</em></p>
</center>
<p><br></p>
<p>Potentially, you can even create attacks knowing <em>just the
training data</em>, even if you have very limited or no access to the
model that you are trying to attack. As of 2023, these kinds of attacks
continue to be a large problem.</p>
<p>To effectively curtail these kinds of black-box attacks, we need to
do two things:</p>
<ol type="1">
<li><strong><em>Really</em> limit who or what can query the
model</strong> and how much. Black boxes with unrestricted API access
are not secure; black boxes with very restricted API access may be.</li>
<li><strong>Hide the training data, while preserving confidence</strong>
that the process used to create the training data is not corrupted.</li>
</ol>
<p>The project that has done the most on the former is perhaps
Worldcoin, of which I analyze an earlier version (among other protocols)
at length <a href="../../../2023/07/24/biometric.html">here</a>.
Worldcoin uses AI models extensively at protocol level, to (i) convert
iris scans into short "iris codes" that are easy to compare for
similarity, and (ii) verify that the thing it's scanning is actually a
human being. The main defense that Worldcoin is relying on is the fact
that <strong>it's not letting anyone simply call into the AI model:
rather, it's using trusted hardware to ensure that the model only
accepts inputs digitally signed by the orb's camera</strong>.</p>
<p>This approach is not guaranteed to work: it turns out that you can
make adversarial attacks against biometric AI that come in the form of
<em>physical patches or jewelry that you can put on your face</em>:</p>
<center>
<p><br></p>
<p><img src="../../../../images/cryptoai/facemask.png" /></p>
<p><em>Wear an extra thing on your forehead, and evade detection or even
impersonate someone else. <a
href="https://arxiv.org/pdf/2109.09320.pdf">Source</a>.</em></p>
</center>
<p><br></p>
<p><strong>But the hope is that if you <em>combine all the defenses
together</em>, hiding the AI model itself, greatly limiting the number
of queries, and requiring each query to somehow be authenticated, you
can make adversarial attacks difficult enough that the system could be
secure.</strong> In the case of Worldcoin, increasing these other
defences could also reduce their dependence on trusted hardware,
increasing the project's decentralization.</p>
<p>And this gets us to the second part: how can we hide the training
data? This is where <strong>"DAOs to democratically govern AI" might
actually make sense</strong>: we can create an on-chain DAO that governs
the process of who is allowed to submit training data (and what
attestations are required on the data itself), who is allowed to make
queries, and how many, and use cryptographic techniques like MPC to
encrypt the entire pipeline of creating and running the AI from each
individual user's training input all the way to the final output of each
query. This DAO could simultaneously satisfy the highly popular
objective of compensating people for submitting data.</p>
<center>
<p><br></p>
<p><img src="../../../../images/cryptoai/hiddennets.png" /></p>
</center>
<p><br></p>
<p>It is important to re-state that this plan is super-ambitious, and
there are a number of ways in which it could prove impractical:</p>
<ul>
<li><strong>Cryptographic overhead could still turn out too
high</strong> for this kind of fully-black-box architecture to be
competitive with traditional closed "trust me" approaches.</li>
<li>It could turn out that <strong>there isn't a good way to make the
training data submission process decentralized <em>and</em>
protected</strong> against poisoning attacks.</li>
<li><strong>Multi-party computation gadgets could break</strong> their
safety or privacy guarantees due to <strong>participants
colluding</strong>: after all, this has happened with cross-chain
cryptocurrency bridges <a
href="https://news.bitcoin.com/harmonys-100m-hack-was-due-to-a-compromised-multi-sig-scheme-says-analyst/">again</a>
and <a
href="https://blockworks.co/news/80-million-lost-orbit-bridge">again</a>.</li>
</ul>
<p>One reason why I didn't start this section with more big red warning
labels saying "DON'T DO AI JUDGES, THAT'S DYSTOPIAN", is that our
society is highly dependent on unaccountable centralized AI judges
already: the algorithms that determine which kinds of posts and
political opinions get boosted and deboosted, or even censored, on
social media. I do think that expanding this trend <em>further</em> at
this stage is quite a bad idea, but I don't think there is a large
chance that <em>the blockchain community experimenting with AIs
more</em> will be the thing that contributes to making it worse.</p>
<p>In fact, there are some pretty basic low-risk ways that crypto
technology can make even these existing centralized systems better that
I am pretty confident in. One simple technique is <strong>verified AI
with delayed publication</strong>: when a social media site makes an
AI-based ranking of posts, it could publish a ZK-SNARK proving the hash
of the model that generated that ranking. The site could commit to
revealing its AI models after eg. a one year delay. Once a model is
revealed, users could check the hash to verify that the correct model
was released, and the community could run tests on the model to verify
its fairness. The publication delay would ensure that by the time the
model is revealed, it is already outdated.</p>
<p>So compared to the <em>centralized</em> world, the question is not
<em>if</em> we can do better, but <em>by how much</em>. For the
<em>decentralized world</em>, however, it is important to be careful:
<strong>if someone builds eg. a prediction market or a stablecoin that
uses an AI oracle, and it turns out that the oracle is attackable,
that's a huge amount of money that could disappear in an
instant</strong>.</p>
<h2 id="ai-as-the-objective-of-the-game">AI as the objective of the
game</h2>
<p>If the above techniques for creating a scalable decentralized private
AI, whose contents are a black box not known by anyone, can actually
work, then this could also be used to create AIs with utility going
beyond blockchains. The NEAR protocol team is making this a <a
href="https://near.org/blog/self-sovereignty-is-near-a-vision-for-our-ecosystem">core
objective of their ongoing work</a>.</p>
<p>There are two reasons to do this:</p>
<ol type="1">
<li>If you <em>can</em> make "<strong>trustworthy black-box
AIs</strong>" by running the training and inference process using some
combination of blockchains and MPC, then lots of applications where
users are worried about the system being biased or cheating them could
benefit from it. Many people have expressed a desire for <strong><a
href="https://openai.com/blog/democratic-inputs-to-ai-grant-program-update">democratic
governance</a> of systemically-important AIs</strong> that we will
depend on; cryptographic and blockchain-based techniques could be a path
toward doing that.</li>
<li>From an <strong>AI safety</strong> perspective, this would be a
technique to create a decentralized AI that also has a natural kill
switch, and which could limit queries that seek to use the AI for
malicious behavior.</li>
</ol>
<p>It is also worth noting that "using crypto incentives to incentivize
making better AI" can be done without also going down the full rabbit
hole of using cryptography to completely encrypt it: approaches like <a
href="https://bittensor.com/whitepaper">BitTensor</a> fall into this
category.</p>
<h2 id="conclusions">Conclusions</h2>
<p>Now that both blockchains and AIs are becoming more powerful, there
is a growing number of use cases in the intersection of the two areas.
However, some of these use cases make much more sense and are much more
robust than others. In general, use cases where the underlying mechanism
continues to be designed roughly as before, but the individual
<em>players</em> become AIs, allowing the mechanism to effectively
operate at a much more micro scale, are the most immediately promising
and the easiest to get right.</p>
<p>The most challenging to get right are applications that attempt to
use blockchains and cryptographic techniques to create a "singleton": a
single decentralized trusted AI that some application would rely on for
some purpose. These applications have promise, both for functionality
and for improving AI safety in a way that avoids the centralization
risks associated with more mainstream approaches to that problem. But
there are also many ways in which the underlying assumptions could fail;
hence, it is worth treading carefully, especially when deploying these
applications in high-value and high-risk contexts.</p>
<p>I look forward to seeing more attempts at constructive use cases of
AI in all of these areas, so we can see which of them are truly viable
at scale.</p>
 </div> 